Metadata-Version: 2.1
Name: SPLADERunner
Version: 0.0.4
Summary: Ultralight and Fast wrapper for the unofficial implementation of SPLADEv2 models for your search & retrieval pipelines. Models and Library created by Prithivi Da, For PRs and Collaboration to checkout the readme.
Home-page: https://github.com/PrithivirajDamodaran/SPLADERunner
Author: Prithivi Da
Author-email: 
License: CC-BY-SA-4.0
Classifier: Programming Language :: Python :: 3
Classifier: Operating System :: OS Independent
Requires-Python: >=3.6
Description-Content-Type: text/markdown
Requires-Dist: tokenizers
Requires-Dist: onnxruntime
Requires-Dist: numpy
Requires-Dist: requests
Requires-Dist: tqdm

# SPLADERunner

Ultra-lite &amp; Super-fast Python wrapper for the unofficial implementation of SPLADEv2 (SParse Lexical And Expanion) models for your search & retrieval pipelines. Implementation is based on SPLADEv2 paper: https://arxiv.org/abs/2109.10086 with some minor tweaks.

1. âš¡ **Ultra-lite & Superfast**: 
    - **No Torch or Transformers** needed. Runs on CPU.
    - Boasts the 
        - **tiniest document expander, ~8MB**.
        - **Faster inference**
    
2. ðŸ’¡ **Why learn Sparse Representations?**:

- Lexical search with BOW models are strong baselines, but they famously suffer from **vocabulary mismatch** problem,as they can only do exact term matching. BoW representations are calculated and not learned.

- Learned (neural) rankers / Dense retrievers with approximate nearest neighbors search has shown impressive results but they can 
    - Suffer from **token amnesia** (sometimes miss simple term matching), 
    - Can be **resource intensive** (both for storage and retreival), 
    - Are famously **not interpretable** and 
    - Scaling to newer domains needs data/finetuning.

- Best of both worlds made sense and gave rise to interest in **learning sparse representations** for queries and documents with some interpretability. The sparse representations also double as implicit or explicit (latent, contextualized) expansion mechanisms for both query and documents.


3. **What the Models learn?**:
- The model learns to project it's learned dense representations over a MLM head to give a vocabulary distribution.
- <center><img src="./images/vocproj.png" width=300/></center>

4. ðŸ’¸ **Why SPLADERunner?**:
    - **$ Concious:** Serverless deployments like Lambda are charged by memory & time per invocation*
    - **Smaller package size** = shorter cold start times, quicker re-deployments for Serverless.
    - **Permissive License**. (You can use it commercially with Source attribution).

5. ðŸŽ¯ **Models**:
    - Below are the list of models supported as of now.
        * [prithivida/Tiny-SPLADE-Doc](https://huggingface.co/prithivida/Tiny-SPLADE-Doc) (default model)
        * [prithivida/Tiny-SPLADE-Query](https://huggingface.co/prithivida/Tiny-SPLADE-Query) (Coming soon)

## ðŸš€ Installation:
```python 
pip install spladerunner
```

## Usage:
```python
#One-time init
from spladerunner import Expander
# Default model is the document expander.
exapander = Expander()

#Sample Document expansion
sparse_rep = expander.expand("Chronic Obstructive Pulmonary Disease (COPD) presents a complex interplay of respiratory and cardiovascular comorbidities, necessitating multidisciplinary management.")
```


4. ðŸ’¸ **Where and How can you use?**:
- [TBD]
