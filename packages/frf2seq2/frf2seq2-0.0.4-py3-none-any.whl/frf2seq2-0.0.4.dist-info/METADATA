Metadata-Version: 2.1
Name: frf2seq2
Version: 0.0.4
Summary: frame2seq test
Home-page: https://github.com/pypa/sampleproject
Author: Deniz Akpinaroglu
Author-email: dakpinaroglu@berkeley.edu
Project-URL: Bug Tracker, https://github.com/pypa/sampleproject/issues
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.10
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: torch >=2.1.1
Requires-Dist: biopython >=1.82
Requires-Dist: einops >=0.7.0
Requires-Dist: pytorch-lightning >=2.1.3
Requires-Dist: scipy >=1.11.4
Requires-Dist: dm-tree >=0.1.8

# Frame2seq
Official repository for Frame2seq, a structured-conditioned masked language model for protein sequence design, as described in our preprint [Structure-conditioned masked language models for protein sequence design generalize beyond the native sequence space](https://doi.org/10.1101/2023.12.15.571823).

## Colab notebook
We provide a [Colab notebook](link) that demonstrates how to use Frame2seq to generate sequences and calculate the pseudo-log-likelihood of a sequence.

## Setup
To use Frame2seq, install via pip:
```bash
pip install frame2seq
```

Alternatively, you can clone this repository and install the package locally:
```bash
$ git clone git@github.com:dakpinaroglu/Frame2seq.git
$ pip install Frame2seq
```

## Usage

### Sequence design

To use Frame2seq to generate sequences, you can use the `generate_sequences` function. This function takes a trained model, a tokenizer, and a list of structures as input, and returns a list of generated sequences. The `generate_sequences` function also takes a number of optional arguments, which are described below.

```python
from frame2seq import generate_sequences

sequences = generate_sequences(
    model=model,
    tokenizer=tokenizer,
    structures=structures,
    num_sequences=10,
    max_length=100,
    num_beams=10,
    num_return_sequences=10,
    temperature=1.0,
    top_k=50,
    top_p=0.95,
    repetition_penalty=1.0,
    length_penalty=1.0,
    no_repeat_ngram_size=0,
    bad_words_ids=None,
    bos_token_id=0,
    eos_token_id=2,
    pad_token_id=1,
    device="cuda",
    seed=42,
)
```

#### Arguments

- `model` (required): A trained model. This can be a `transformers.PreTrainedModel` or a `torch.nn.Module`.
- `tokenizer` (required): A tokenizer. This can be a `transformers.PreTrainedTokenizer` or a `tokenizers.Tokenizer`.
- `structures` (required): A list of structures. Each structure should be a `Bio.PDB.Structure.Structure` object.
- `num_sequences` (optional): The number of sequences to generate per structure. Defaults to 10.
- `max_length` (optional): The maximum length of the generated sequences. Defaults to 100.
- `num_beams` (optional): The number of beams to use for beam search. Defaults to 10.
- `num_return_sequences` (optional): The number of sequences to return per structure. Defaults to 10.
- `temperature` (optional): The temperature to use for sampling. Defaults to 1.0.
- `top_k` (optional): The number of highest probability vocabulary tokens to keep for top-k-filtering. Defaults to 50.
- `top_p` (optional): The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Defaults to 0.95.

### Pseudo-log-likelihood

To use Frame2seq to calculate the pseudo-log-likelihood of a sequence, you can use the `pseudo_log_likelihood` function. This function takes a trained model, a tokenizer, a structure, and a sequence as input, and returns the pseudo-log-likelihood of the sequence. The `pseudo_log_likelihood` function also takes a number of optional arguments, which are described below.

```python
from frame2seq import pseudo_log_likelihood

pseudo_log_likelihood = pseudo_log_likelihood(
    model=model,
    tokenizer=tokenizer,
    structure=structure,
    sequence=sequence,
    num_samples=10,
    max_length=100,
    temperature=1.0,
    top_k=50,
    top_p=0.95,
    repetition_penalty=1.0,
    length_penalty=1.0,
    no_repeat_ngram_size=0,
    bad_words_ids=None,
    bos_token_id=0,
    eos_token_id=2,
    pad_token_id=1,
    device="cuda",
    seed=42,
)
```

## Citing this work

```bibtex
@article{akpinaroglu2023structure,
  title={Structure-conditioned masked language models for protein sequence design generalize beyond the native sequence space},
  author={Akpinaroglu, Deniz and Seki, Kosuke and Guo, Amy and Zhu, Eleanor and Kelly, Mark JS and Kortemme, Tanja},
  journal={bioRxiv},
  pages={2023--12},
  year={2023},
  publisher={Cold Spring Harbor Laboratory}
}
```
