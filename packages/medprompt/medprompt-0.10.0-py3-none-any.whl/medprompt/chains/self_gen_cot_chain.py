"""
 Copyright 2023 Bell Eapen

 Licensed under the Apache License, Version 2.0 (the "License");
 you may not use this file except in compliance with the License.
 You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

 Unless required by applicable law or agreed to in writing, software
 distributed under the License is distributed on an "AS IS" BASIS,
 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 See the License for the specific language governing permissions and
 limitations under the License.
"""


import logging
from langchain.prompts.prompt import PromptTemplate
from langchain.schema.output_parser import StrOutputParser
from langchain.schema.runnable import RunnablePassthrough
from langchain.tools import tool
from langchain_core.pydantic_v1 import BaseModel, Field
from overrides import override

_logger = logging.getLogger(__name__)

from .base_medprompt_chain import BaseMedpromptChain


class SelfGenCotChain(BaseMedpromptChain):

    class SelfGenCotInput(BaseModel):
        question: str = Field()
        answer: str = Field()

    def __init__(self,
                    main_llm=None,
                    clinical_llm=None,
                    ):
        super().__init__(
            main_llm=main_llm,
            clinical_llm=clinical_llm,
        )
        self._name = "self generated explanation"
        self._input_type = self.SelfGenCotInput
        SELF_GENERATED_COT_TEMPLATE = """
        ## Question: {question}
        ## Answer: {answer}
        Given the above question and answer, generate a chain of thought explanation for the answer.
        First, start with the model generated chain of thought explanation.
        End the chain of though explanation with:
        Therefore, the answer is {answer}.
        """

        self.SELF_GENERATED_COT_PROMPT = PromptTemplate.from_template(SELF_GENERATED_COT_TEMPLATE)

    @property
    @override
    def chain(self):
        """Get the runnable chain."""
        _cot = RunnablePassthrough.assign(
            question = lambda x: x["question"],
            answer = lambda x: x["answer"],
            ) | self.SELF_GENERATED_COT_PROMPT | self.main_llm | StrOutputParser()
        chain = _cot.with_types(input_type=self.input_type)
        return chain


@tool(SelfGenCotChain().name, args_schema=SelfGenCotChain().input_type)
def get_tool(**kwargs):
    """
    Returns the self generated chain of thought explanation for the given question and answer.

    The input is a dict with the following keys:
        question (str): The question asked to the model.
        answer (str): The answer generated by the model.
    """
    return SelfGenCotChain().chain.invoke(kwargs)