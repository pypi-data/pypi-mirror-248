{
    "name": "LangchainSet",
    "description": "Test suite on the langchain project.",
    "repo_url": "https://github.com/langchain-ai/langchain.git",
    "cases": [
        {
            "id": "langchain-2",
            "question": "What classes are derived from the `Chain` class?",
            "reference_answer": "There are multiple classes that are derived from the Chain class. Some of them are:\n\nAPIChain\nAnalyzeDocumentChain\nChatVectorDBChain\nCombineDocumentsChain\nConstitutionalChain\nConversationChain\nGraphQAChain\nHypotheticalDocumentEmbedder\nLLMChain\nLLMCheckerChain\nLLMRequestsChain\nLLMSummarizationCheckerChain\nMapReduceChain\nOpenAPIEndpointChain\nPALChain\nQAWithSourcesChain\nRetrievalQA\nRetrievalQAWithSourcesChain\nSequentialChain\nSQLDatabaseChain\nTransformChain\nVectorDBQA\nVectorDBQAWithSourcesChain\n\nThere might be more classes that are derived from the Chain class as it is possible to create custom classes that extend the Chain class.",
            "answer_properties": [],
            "notes": {
                "level": "high"
            }
        },
        {
            "id": "langchain-3",
            "question": "What classes and functions in the `./langchain/utilities/` forlder are not covered by unit tests?",
            "reference_answer": "All classes and functions in the ./langchain/utilities/ folder seem to have unit tests written for them.",
            "answer_properties": [],
            "notes": {
                "level": "mid"
            }
        },
        {
            "id": "langchain-5",
            "question": "Does `UnstructuredMarkdownLoader` accept a string representation of a markdown file?",
            "reference_answer": "No, UnstructuredMarkdownLoader does not accept a string representation of a markdown file. It takes a file path as input.",
            "answer_properties": [],
            "notes": {
                "level": "mid"
            }
        },
        {
            "id": "langchain-7",
            "question": "Is OpenAI's `gpt-4-0613` model supported?",
            "reference_answer": "No, this model is not supported.",
            "answer_properties": [],
            "notes": {
                "level": "mid"
            }
        },
        {
            "id": "langchain-8",
            "question": "How do I use an `LLMChain` with a chat model? Please provide example code.",
            "reference_answer": "The LLMChain can be used with chat models as well:\n```\nfrom langchain import LLMChain\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    SystemMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n)\n\nchat = ChatOpenAI(temperature=0)\n\ntemplate = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\nsystem_message_prompt = SystemMessagePromptTemplate.from_template(template)\nhuman_template = \"{text}\"\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\nchat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n\nchain = LLMChain(llm=chat, prompt=chat_prompt)\nchain.run(input_language=\"English\", output_language=\"French\", text=\"I love programming.\")\n```",
            "answer_properties": [],
            "notes": {
                "level": "high"
            }
        },
        {
            "id": "langchain-9",
            "question": "Tell me about the `QAGenerationChain`.",
            "reference_answer": "The QAGenerationChain is a chain in LangChain that generates question-answer pairs over a specific document. It is a useful tool for generating evaluation data for question-answering tasks when you don't have a large dataset of examples. The QAGenerationChain takes a document as input and generates question-answer pairs that can be used to evaluate question-answering systems. This allows you to evaluate the performance of your question-answering system without the need for a large amount of pre-existing data.",
            "answer_properties": [],
            "notes": {
                "level": "low"
            }
        },
        {
            "id": "langchain-11",
            "question": "What does the `max_marginal_relevance_search` function do?",
            "reference_answer": "The `max_marginal_relevance_search` function returns a list of documents selected using the maximal marginal relevance algorithm, which optimizes for similarity to the query and diversity among the selected documents.",
            "answer_properties": [],
            "notes": {
                "level": "low"
            }
        },
        {
            "id": "langchain-12",
            "question": "What is the purpose of the `Qdrant` class?",
            "reference_answer": "The purpose of the Qdrant class is to serve as a wrapper around the Qdrant vector database.",
            "answer_properties": [],
            "notes": {
                "level": "mid"
            }
        },
        {
            "id": "langchain-13",
            "question": "What does the `save_context` method of ConversationEntityMemory do?",
            "reference_answer": "The `save_context` method saves the context from the conversation history to the entity store by generating summaries for each entity in the entity cache.",
            "answer_properties": [],
            "notes": {
                "level": "mid"
            }
        },
        {
            "id": "langchain-14",
            "question": "What is the use of the `ConversationBufferMemory` class?",
            "reference_answer": "The purpose of the ConversationBufferMemory class is to store conversation memory in a string buffer.",
            "answer_properties": [],
            "notes": {
                "level": "mid"
            }
        },
        {
            "id": "langchain-15",
            "question": "What is a prompt template?",
            "reference_answer": "A prompt template refers to a reproducible way to generate a prompt. It contains a text string (\"the template\") that can take in a set of parameters from the end user and generates a prompt. A prompt template can include instructions to the language model, a set of few-shot examples to help the language model generate a better response, and a question to the language model. Prompt templates can be created using the PromptTemplate class in LangChain.",
            "answer_properties": [],
            "notes": {
                "level": "mid"
            }
        },
        {
            "id": "langchain-16",
            "question": "Is there a caching mechanism for LLMs?",
            "reference_answer": "Yes, there is a caching mechanism for LLMs (Language Models). Caching can be used to save money by reducing the number of API calls made to the LLM provider and to speed up applications by reducing the number of API calls. There are different caching options available such as In Memory Cache, SQLite Cache, Redis Cache, Semantic Cache, GPTCache, Momento Cache, and SQLAlchemy Cache. These caching options can be used to cache prompts and responses, and some of them even support caching based on semantic similarity. Additionally, caching can be turned off for specific LLMs if desired.",
            "answer_properties": [],
            "notes": {
                "level": "mid"
            }
        },
        {
            "id": "langchain-17",
            "question": "Give me a list of the main components of Langchain.",
            "reference_answer": "The main components of LangChain are as follows:\n\nModels: LangChain supports various model types and model integrations.\nPrompts: This module includes prompt management, prompt optimization, and prompt serialization.\nMemory: LangChain provides a standard interface for memory, a collection of memory implementations, and examples of chains/agents that use memory.\nIndexes: This module covers best practices for combining language models with your own text data.\nChains: LangChain provides a standard interface for chains, integrations with other tools, and end-to-end chains for common applications.\nAgents: LangChain provides a standard interface for agents, a selection of agents to choose from, and examples of end-to-end agents.\n\nThese components can be used in various ways, including for personal assistants, question answering, chatbots, querying tabular data, interacting with APIs, extraction, summarization, and evaluation. Additionally, LangChain provides reference documentation for all its features and an ecosystem for integrating with other companies/products.",
            "answer_properties": [],
            "notes": {
                "level": "high"
            }
        },
        {
            "id": "langchain-18",
            "question": "Difference b/t a react agent and a plan-and-execute agent?",
            "reference_answer": "A ReAct agent and a plan-and-execute agent are two different types of agents in the LangChain framework.\n\nA ReAct agent, which stands for \"Reaction Agent,\" uses the ReAct framework to determine which tool to use based on the tool's description. It can use multiple tools and is designed to be used in conversational settings. It uses memory to remember previous conversation interactions and decides the next step using the history of tools, tool inputs, and observations. ReAct agents are suitable for small tasks and are wrapped in agent executors.\n\nOn the other hand, a plan-and-execute agent plans the full sequence of steps to accomplish an objective and then executes those steps in order. The planning is usually done by a language model (LLM), and the execution is done by a separate agent equipped with tools. Plan-and-execute agents are better suited for complex or long-running tasks that require maintaining long-term objectives and focus.\n\nIn summary, a ReAct agent uses the ReAct framework to decide which tool to use based on the tool's description and is designed for conversational settings. A plan-and-execute agent plans the full sequence of steps and executes them in order, making it suitable for complex tasks.",
            "answer_properties": [],
            "notes": {
                "level": "high"
            }
        },
        {
            "id": "langchain-19",
            "question": "Is there a `SelfQueryRetriever` for `Milvus`?",
            "reference_answer": "No. Self querying for Milvus is not supported.",
            "answer_properties": [],
            "notes": {
                "level": "mid"
            }
        },
        {
            "id": "langchain-20",
            "question": "How do I define a custom tool?",
            "reference_answer": "To define a custom tool, you can use either the Tool dataclass or subclass the BaseTool class.\n\nUsing the Tool dataclass:\nImport the necessary modules: from langchain import Tool\nDefine the tool using the Tool dataclass, providing the required components such as name, description, and function.\nOptionally, you can define an args_schema to provide more information or validation for expected parameters.\nSubclassing the BaseTool class:\nImport the necessary modules: from langchain import BaseTool\nCreate a custom class that subclasses BaseTool.\nImplement the necessary methods and instance variables in your custom class.\nAdditionally, there is a @tool decorator provided that can be used to quickly create a Tool from a simple function. The decorator uses the function name as the tool name by default and the function's docstring as the tool's description.",
            "answer_properties": [],
            "notes": {
                "level": "mid"
            }
        },
        {
            "id": "langchain-21",
            "question": "Is there an agent type that supports OpenAI's function calling interface?",
            "reference_answer": "Yes, there is an agent type that supports OpenAI's function calling interface. It is called the OpenAI Functions Agent. This agent is designed to work with OpenAI models that have been fine-tuned to detect when a function should be called and respond with the inputs that should be passed to the function. The OpenAI Functions Agent is capable of responding to prompts from the user using a Large Language Model.",
            "answer_properties": [],
            "notes": {
                "level": "mid"
            }
        }
    ]
}