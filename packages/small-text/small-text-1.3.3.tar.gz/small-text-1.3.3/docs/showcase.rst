========
Showcase
========

Papers
------

- | Hannah Kirk, Bertie Vidgen, and Scott Hale. 2022.
  | `Is More Data Better? Re-thinking the Importance of Efficiency in Abusive Language Detection with Transformers-Based Active Learning. <https://aclanthology.org/2022.trac-1.7/>`_
  | In Proceedings of the Third Workshop on Threat, Aggression and Cyberbullying (TRAC 2022), pages 52â€“61, Gyeongju, Republic of Korea. Association for Computational Linguistics.

- | Julius Gonsior, Christian Falkenberg, Silvio Magino, Anja Reusch, Maik Thiele, and Wolfgang Lehner. 2022.
  | `To Softmax, or not to Softmax: that is the question when applying Active Learning for Transformer Models. <https://arxiv.org/abs/2210.03005>`_
  | ArXiv, abs/2210.03005.

- | Julia Romberg and Tobias Escher. 2022.
  | `Automated Topic Categorisation of Citizensâ€™ Contributions: Reducing Manual Labelling Efforts Through Active Learning. <https://link.springer.com/chapter/10.1007/978-3-031-15086-9_24>`_
  | In EGOV 2022: Electronic Government, pages 369â€“385, Cham. Springer International Publishing.

- | Christopher SchrÃ¶der, Andreas Niekler, and Martin Potthast. 2022.
  | `Revisiting Uncertainty-based Query Strategies for Active Learning with Transformers. <https://aclanthology.org/2022.findings-acl.172/>`_
  | In Findings of the Association for Computational Linguistics: ACL 2022, pages 2194â€“2203.

Tutorials
---------

- | `ðŸ‘‚ Active learning for text classification with small-text <https://docs.argilla.io/en/latest/tutorials/notebooks/training-textclassification-smalltext-activelearning.html>`_
