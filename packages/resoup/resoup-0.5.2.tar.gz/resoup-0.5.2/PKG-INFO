Metadata-Version: 2.1
Name: resoup
Version: 0.5.2
Summary: Various convenient features related to requests.
License: MIT
Keywords: requests,bs4,BeautifulSoup,async,caching,cache
Author: ilotoki0804
Author-email: ilotoki0804@gmail.com
Requires-Python: >=3.10,<4.0
Classifier: Environment :: Web Environment
Classifier: Framework :: AsyncIO
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Topic :: Internet :: WWW/HTTP
Requires-Dist: beautifulsoup4 (>=4.12.2,<5.0.0)
Requires-Dist: frozendict (>=2.3.10,<3.0.0)
Requires-Dist: requests (>=2.31.0,<3.0.0)
Requires-Dist: typing-extensions (>=4.8.0,<5.0.0)
Project-URL: Changelog, https://github.com/ilotoki0804/resoup#relese-note
Project-URL: Documentation, https://github.com/ilotoki0804/resoup
Project-URL: Issues, https://github.com/ilotoki0804/resoup/issues
Project-URL: Repository, https://github.com/ilotoki0804/resoup
Description-Content-Type: text/markdown

ì´ ì„¤ëª…ì€ ìµœì‹  ë²„ì „ì´ ì•„ë‹ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë§Œì•½ ìµœì‹  ë²„ì „ì„ í™•ì¸í•˜ê³  ì‹¶ìœ¼ì‹œë‹¤ë©´ [ì´ ê¹ƒí—ˆë¸Œ ë§í¬](https://github.com/ilotoki0804/resoup)ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.
# resoup

**Various convenient features related to requests and BeautifulSoup.** (<span style="color:blue">**_re_**</span>quests + Beautiful<span style="color:blue">**_Soup_**</span>)

1. `requests`ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ BeatifulSoupë¥¼ í•©ì³ ëª‡ ì¤„ì˜ ì½”ë“œë¥¼ í•˜ë‚˜ì— í•©ì¹  ìˆ˜ ìˆìœ¼ë©°,
1. ê°„ë‹¨í•˜ê²Œ async, cacheë¥¼ ë¶ˆëŸ¬ì™€ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
1. ì›¹ ìŠ¤í¬ë˜í•‘ ì‹œ í¸ë¦¬í•œ ê¸°ë³¸ê°’ë„ ì¤€ë¹„ë˜ì–´ ìˆê³ ,
1. `no_empty_result`, `attempts`, `avoid_sslerror` ë“± ë‹¤ì–‘í•˜ê³  ì†Œì†Œí•œ ê¸°ëŠ¥ë„ ì¤€ë¹„ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

ì†Œì†Œí•˜ì§€ë§Œ ìœ ìš©í•˜ë©°, ì„œë„ˆ ì¤„ì˜ ì½”ë“œ ì‘ì„±ëŸ‰ì„ ì¤„ì—¬ì£¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.

## ì‹œì‘í•˜ê¸°

1. íŒŒì´ì¬ì„ ì„¤ì¹˜í•©ë‹ˆë‹¤.
1. í„°ë¯¸ë„ì—ì„œ ë‹¤ìŒê³¼ ê°™ì€ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.

   ```console
   pip install -U resoup
   ```

requestsì™€ bs4ëŠ” ê°™ì´ ì„¤ì¹˜ë˜ì§€ë§Œ BeatifulSoupì˜ ì¶”ê°€ì ì¸ parserì¸ lxmlì™€ html5libëŠ” ê¸°ë³¸ìœ¼ë¡œ ì œê³µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

ë”°ë¼ì„œ lxml, html5lib ë“±ì€ ìŠ¤ìŠ¤ë¡œ ì„¤ì¹˜í•˜ì…”ì•¼ ì˜¤ë¥˜ê°€ ë‚˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ë§Œì•½ ì„¤ì¹˜ë˜ì§€ ì•Šì€ ìƒíƒœë¡œ í•´ë‹¹ parserë¥¼ ì´ìš©í•œë‹¤ë©´ `NoParserError`ê°€ ë‚©ë‹ˆë‹¤.

## ì‚¬ìš©ë²•

ì°¸ê³ : ì˜ˆì‹œë“¤ì˜ ê²½ìš° ë§ì€ ê²½ìš° `get` ìš”ì²­ì„ ìœ„ì£¼ë¡œ ì„¤ëª…í•˜ì§€ë§Œ, ë‹¤ë¥¸ ëª¨ë“  ë©”ì†Œë“œ(options/head/post/put/patch/delete)ì—ì„œë„ ë™ì¼í•˜ê²Œ ì‘ë™í•©ë‹ˆë‹¤.

### `resoup.requests` ëª¨ë“ˆ

`resoup.requests` ëª¨ë“ˆì€ ë‹¤ìŒê³¼ ê°™ì´ importí•´ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
from resoup import requests  # `import requests`ì™€ í˜¸í™˜ë¨.
```

ì´ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” requests ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ 99% í˜¸í™˜ë˜ë©° (ì‹¬ì§€ì–´ íƒ€ì… íŒíŠ¸ë„ requests ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ ë˜‘ê°™ì´ ì˜ ì‘ë™í•©ë‹ˆë‹¤!), ê·¸ ìœ„ì— í¸ë¦¬í•œ ê¸°ëŠ¥ì„ ì–¹ì€ í˜•íƒœì…ë‹ˆë‹¤. ì¦‰, ê¸°ì¡´ `import requests`ë¥¼ ìœ„ì˜ ì½”ë“œë¡œ êµì²´í•˜ë©´ ê¸°ì¡´ì˜ ì½”ë“œë¥¼ ë§ê°€ëœ¨ë¦¬ì§€ ì•Šìœ¼ë©´ì„œë„ ì˜ í†µí•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

requestsì˜ Sessionë„ ë¹„ìŠ·í•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
from resoup import requests

with requests.Session() as session:
    ...  # cget, attempts ë“± ëª¨ë“  ê¸°ëŠ¥ ì‚¬ìš© ê°€ëŠ¥
```

#### ê¸°ë³¸ê°’

ê¸°ë³¸ê°’ë“¤ì€ ê°ê° ì ë‹¹í•œ ê°’ìœ¼ë¡œ ì„¤ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

ê¸°ë³¸ê°’ë“¤ì€ ë‹¤ìŒê³¼ ê°™ê³  request.get/options/head/post/put/patch/deleteì—ì„œ ì ìš©ë©ë‹ˆë‹¤.

```python
timeout ê¸°ë³¸ê°’: 120
headers ê¸°ë³¸ê°’: {
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
    "Accept-Encoding": "gzip, deflate, br",
    "Accept-Language": "ko-KR,ko;q=0.9",
    "Sec-Ch-Ua": '"Chromium";v="116", "Not)A;Brand";v="24", "Google Chrome";v="116"',
    "Sec-Ch-Ua-Mobile": "?0",
    "Sec-Ch-Ua-Platform": '"Windows"',
    "Sec-Fetch-Dest": "document",
    "Sec-Fetch-Mode": "navigate",
    "Sec-Fetch-Site": "none",
    "Sec-Fetch-User": "?1",
    "Upgrade-Insecure-Requests": "1",
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36",
}
attempts ê¸°ë³¸ê°’: 1
avoid_sslerror ê¸°ë³¸ê°’: False
```

```python
>>> from resoup import requests
>>>
>>> from resoup import requests
>>> res = requests.get("https://httpbin.org/headers")
>>> res.json()['headers']
{'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
 'Accept-Encoding': 'gzip, deflate, br',
 'Accept-Language': 'ko-KR,ko;q=0.9',
 'Host': 'httpbin.org',
 'Sec-Ch-Ua': '"Chromium";v="116", "Not)A;Brand";v="24", "Google Chrome";v="116"',
 'Sec-Ch-Ua-Mobile': '?0',
 'Sec-Ch-Ua-Platform': '"Windows"',
 'Sec-Fetch-Dest': 'document',
 'Sec-Fetch-Mode': 'navigate',
 'Sec-Fetch-Site': 'none',
 'Sec-Fetch-User': '?1',
 'Upgrade-Insecure-Requests': '1',
 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36',
 'X-Amzn-Trace-Id': ...}
```

#### ì‘ë‹µ

`resoup.requests` ëª¨ë“ˆì˜ get/options/head/post/put/patch/delete í•¨ìˆ˜ëŠ” ëª¨ë‘ ResponseProxyë¥¼ ë¦¬í„´í•©ë‹ˆë‹¤.

ResponseProxyëŠ” ê¸°ì¡´ Responseì™€ 100% í˜¸í™˜ë˜ëŠ” Responseì˜ subclassì…ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ `ResponseProxy` í•­ëª©ì„ ì°¸ê³ í•˜ì„¸ìš”.

ê¸°ëŠ¥ì„ ì˜ ì´í•´í•˜ì§€ ëª»í–ˆë‹¤ë©´ ê¸°ì¡´ì— Responseë¥¼ ì‚¬ìš©í•˜ë˜ ë°©ì‹ëŒ€ë¡œ ì‚¬ìš©í•˜ì‹œë©´ ë¬¸ì œ ì—†ì´ ì‘ë™í•©ë‹ˆë‹¤.

#### attempts

`attempts`ëŠ” íŒŒë¼ë¯¸í„°ë¡œ, ëª¨ì¢…ì˜ ì´ìœ ë¡œ `ConnectionError`ê°€ ë°œìƒí–ˆì„ ë•Œ ê°™ì€ requestsë¥¼ ëª‡ ë²ˆ ë” ë°˜ë³µí•  ê²ƒì¸ì§€ ì„¤ì •í•˜ëŠ” íŒŒë¼ë¯¸í„°ì…ë‹ˆë‹¤.

ë§Œì•½ 10ë²ˆì„ ì‹¤í–‰í•˜ê³ ë„ ì‹¤íŒ¨í–ˆë‹¤ë©´ ê°€ì¥ ìµœê·¼ì— ì‹¤íŒ¨í•œ ì—°ê²°ì˜ ì´ìœ ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.

```python
>>> from resoup import requests
>>>
>>> requests.get('https://some-not-working-website.com', attempts=10)
WARNING:root:Retring...
WARNING:root:Retring...
WARNING:root:Retring...
WARNING:root:Retring...
WARNING:root:Retring...
WARNING:root:Retring...
WARNING:root:Retring...
WARNING:root:Retring...
WARNING:root:Retring...
WARNING:root:Retring...
Traceback (most recent call last):
...
socket.gaierror: [Errno 11001] getaddrinfo failed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
...
urllib3.exceptions.NameResolutionError: <urllib3.connection.HTTPSConnection object at ...>: Failed to resolve 'some-not-working-website.com' ([Errno 11001] getaddrinfo failed)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
...
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='some-not-working-website.com', port=443): Max retries exceeded with url: / (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at ...>: Failed to resolve 'some-not-working-website.com' ([Errno 11001] getaddrinfo failed)"))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
...
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='some-not-working-website.com', port=443): Max retries exceeded with url: / (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at ...>: Failed to resolve 'some-not-working-website.com' ([Errno 11001] getaddrinfo failed)"))

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
...
ConnectionError: Trying 10 times but failed to get data.
URL: https://some-not-working-website.com
```

### avoid_sslerror

`avoid_sslerror`ëŠ” `UNSAFE_LEGACY_RENEGOTIATION_DISABLED`ìœ¼ë¡œ ì¸í•´ ì˜¤ë¥˜ê°€ ë‚˜íƒ€ë‚˜ëŠ” ì‚¬ì´íŠ¸ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì˜ˆë¥¼ ë“¤ì–´ ë‹¤ìŒì˜ ì‚¬ì´íŠ¸ëŠ” `avoid_sslerror` ì—†ì´ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì˜¤ë¥˜ë¥¼ ì¼ìœ¼í‚µë‹ˆë‹¤.

```python
>>> from resoup import requests
>>> requests.get('https://bufftoon.plaync.com')
---------------------------------------------------------------------------
SSLError                                  Traceback (most recent call last)
...
SSLError: HTTPSConnectionPool(host='bufftoon.plaync.com', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1000)')))
```

`avoid_sslerror`ë¥¼ `True`ë¡œ í•˜ë©´ í•´ë‹¹ ì˜¤ë¥˜ë¥¼ í”¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
<Response [200]>
```

#### ì¼ë°˜ ìš”ì²­ í•¨ìˆ˜

ì¼ë°˜ requests.get/options/head/post/put/patch/deleteë¥¼ `requests`ì—ì„œ ì‚¬ìš©í•˜ë˜ ë°©ì‹ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë‹¤ìŒì€ requests.getê³¼ postì˜ ì˜ˆì‹œì…ë‹ˆë‹¤. `requests`ëª¨ë“ˆê³¼ ë˜‘ê°™ì´ ì‘ë™í•©ë‹ˆë‹¤.

```python
>>> from resoup import requests
>>>
>>> requests.get('https://jsonplaceholder.typicode.com/todos/1').json()  # API that can send request in order to test. Don't execute this command unless you trust this API.
{'userId': 1, 'id': 1, 'title': 'delectus aut autem', 'completed': False}
>>> requests.post('https://jsonplaceholder.typicode.com/todos', json={
...     'title': 'foo',
...     'body': 'bar',
...     'userId': 1,
... }).json()
{'title': 'foo', 'body': 'bar', 'userId': 1, 'id': 201}  # Same with original requests library
```

#### ìºì‹œëœ ìš”ì²­ í•¨ìˆ˜

ì¼ë°˜ requests.get/../delete ìš”ì²­ê³¼ ë™ì¼í•˜ì§€ë§Œ ìºì‹œë©ë‹ˆë‹¤. ì´ë•Œ ìºì‹œëŠ” í›„ìˆ í•  `ë¹„ë™ê¸°ì ì´ë©° ìºì‹œëœ ìš”ì²­ í•¨ìˆ˜`ì™€ ê³µìœ ë©ë‹ˆë‹¤. í•˜ì§€ë§Œ ê° ë©”ì†Œë“œë“¤ë¼ë¦¬ ê³µìœ ë˜ì§€ëŠ” ì•ŠìŠµë‹ˆë‹¤. ì•ì— `c`ë¥¼ ë¶™ì—¬ requests.cget/coptions/chead/cpost/cput/cpatch/cdeleteë¡œ í•¨ìˆ˜ë¥¼ ì‘ì„±í•´ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ê°™ì€ URLì„ ë³´ë‚´ë„ ë‹¤ë¥¸ ê²°ê³¼ë¥¼ ì‘ë‹µí•  ìˆ˜ ìˆëŠ” ë™ì ì¸ ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜(ì‹œê°„ì— ë”°ë¥¸ ì‘ë‹µì˜ ë³€í™”ë¥¼ ë°˜ì˜í•˜ì§€ ì•ŠìŒ) ì‘ë‹µì˜ í¬ê¸°ê°€ í´ ê²½ìš°(ë©”ëª¨ë¦¬ê°€ ë‚­ë¹„ë  ìˆ˜ ìˆìŒ) ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.

```python
>>> # ê¸°ê¸° ì‚¬ì–‘ê³¼ ì¸í„°ë„· ì—°ê²° í’ˆì§ˆì— ë”°ë¼ ê²°ê³¼ëŠ” ë‹¤ë¥¼ ìˆ˜ ìˆìŒ
>>> import timeit
>>>
>>> timeit.timeit('requests.get("https://python.org")', number=10, setup='from resoup import requests')
1.1833231999917189 # ê¸°ê¸° ì‚¬ì–‘ê³¼ ì¸í„°ë„· ì—°ê²° í’ˆì§ˆì— ë”°ë¼ ë‹¤ë¦„: 10ë²ˆì˜ ì—°ê²° ëª¨ë‘ requestë¥¼ ë³´ëƒ„
>>> timeit.timeit('requests.cget("https://python.org")', number=10, setup='from resoup import requests')
0.10267569999268744 # : ì²˜ìŒ í•œ ë²ˆë§Œ requestë¥¼ ë³´ë‚´ê³  ê·¸ ë’¤ëŠ” ìºì‹œì—ì„œ ê°’ì„ ë¶ˆëŸ¬ì˜´
```

#### ë¹„ë™ê¸°ì ì¸ ìš”ì²­ í•¨ìˆ˜

ë¹„ë™ê¸°ì ì¸ ìš”ì²­ì„ ë³´ëƒ…ë‹ˆë‹¤. ì•ì— `a`ë¥¼ ë¶™ì—¬ requests.aget/aoptions/ahead/apost/aput/apatch/adeleteë¡œ í•¨ìˆ˜ë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.

`run_in_executer`ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ì¼œì ¸ ìˆìŠµë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ ì•„ë˜ì˜ `run_in_executer ì‚¬ìš©`ì„ ì°¸ê³ í•˜ì„¸ìš”.

```python
>>> import asyncio
>>> 
>>> from resoup import requests
>>>
>>> res = asyncio.run(requests.aget('https://python.org'))
>>> res
<response [200]>
```

#### ë¹„ë™ê¸°ì ì´ë©° ìºì‹œëœ ìš”ì²­ í•¨ìˆ˜

ë¹„ë™ê¸°ì ì´ë©° ìºì‹œë˜ëŠ” ìš”ì²­ì…ë‹ˆë‹¤. ì´ë•Œ ìºì‹œëŠ” ê°™ì€ ë©”ì†Œë“œë¼ë©´ `ìºì‹œëœ ìš”ì²­ í•¨ìˆ˜`ì™€ ê³µìœ ë©ë‹ˆë‹¤. ì•ì— `ac`ë¥¼ ë¶™ì—¬ requests.acget/acoptions/achead/acpost/acput/acpatch/acdeleteë¡œ í•¨ìˆ˜ë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.

ê°™ì€ URLì„ ë³´ë‚´ë„ ë‹¤ë¥¸ ê²°ê³¼ë¥¼ ì‘ë‹µí•  ìˆ˜ ìˆëŠ” ë™ì ì¸ ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜(ì‹œê°„ì— ë”°ë¥¸ ì‘ë‹µì˜ ë³€í™”ë¥¼ ë°˜ì˜í•˜ì§€ ì•ŠìŒ) ì‘ë‹µì˜ í¬ê¸°ê°€ í´ ê²½ìš°(ë©”ëª¨ë¦¬ê°€ ë‚­ë¹„ë  ìˆ˜ ìˆìŒ) ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.

`run_in_executer`ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ì¼œì ¸ ìˆìŠµë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ ì•„ë˜ì˜ `run_in_executer ì‚¬ìš©`ì„ ì°¸ê³ í•˜ì„¸ìš”.

```python
>>> import asyncio
>>> import timeit
>>>
>>> timeit.timeit('asyncio.run(requests.aget("https://python.org"))', number=10, setup='from resoup import requests; import asyncio')
0.8676127000362612 # ê¸°ê¸° ì‚¬ì–‘ê³¼ ì¸í„°ë„· ì—°ê²° í’ˆì§ˆì— ë”°ë¼ ë‹¤ë¦„: 10ë²ˆì˜ ì—°ê²° ëª¨ë‘ requestë¥¼ ë³´ëƒ„
>>> timeit.timeit('asyncio.run(requests.acget("https://python.org"))', number=10, setup='from resoup import requests; import asyncio')
0.11984489997848868 # ì²˜ìŒ í•œ ë²ˆë§Œ requestë¥¼ ë³´ë‚´ê³  ê·¸ ë’¤ëŠ” ìºì‹œë¥¼ ë¶ˆëŸ¬ì˜´
```

#### `run_in_executer` ì‚¬ìš©

ë¹„ë™ê¸°ì ì¸ ìš”ì²­(aget, acget ë“± aê°€ ë¶™ì€ ë©”ì†Œë“œ)ì—ì„œëŠ” `run_in_executer` parameterë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ parameterëŠ” í•¨ìˆ˜ê°€ ë‹¤ë¥¸ ì“°ë ˆë“œì—ì„œ ëŒê²Œ í•©ë‹ˆë‹¤. ìˆœì°¨ì ìœ¼ë¡œ í”„ë¡œê·¸ë¨ì´ ë™ì‘í•  ë•Œì—ëŠ” í° ì°¨ì´ê°€ ì—†ì§€ë§Œ ë³‘ë ¬ì ìœ¼ë¡œ í”„ë¡œê·¸ë¨ì„ ëŒë¦´ ë•Œ í° ì†ë„ í–¥ìƒì„ ê¸°ëŒ€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì•„ë˜ì™€ ê°™ì´ `asyncio.gather`ë¥¼ ì´ìš©í•˜ë©´ í° ì„±ëŠ¥ í–¥ìƒì„ ë³´ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
import asyncio
import time

from resoup import requests

async def masure_coroutine_time(coroutine):
    start = time.perf_counter()
    await coroutine
    end = time.perf_counter()

    print(end - start)

async def main():
    # ë‹¨ì¼ requestë¥¼ ë³´ë‚¼ ë•Œ(í° ì°¨ì´ ì—†ìŒ)

    req = requests.aget('https://python.org', run_in_executor=False)
    await masure_coroutine_time(req)  # 0.07465070000034757

    req = requests.aget('https://python.org')
    await masure_coroutine_time(req)  # 0.05844969999452587

    # ì—¬ëŸ¬ requestë¥¼ ë³´ë‚¼ ë•Œ(í° ì†ë„ í–¥ìƒì„ ë³´ì„)

    reqs = (requests.aget(f'https://python.org/{i}', run_in_executor=False) for i in range(10))  # ë”ë¯¸ urlì„ ë§Œë“¦
    await masure_coroutine_time(asyncio.gather(*reqs))  # run_in_executorë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì„ ë•Œ: ëŠë¦¼(3.7874760999984574)

    reqs = (requests.aget(f'https://python.org/{i}') for i in range(10))  # ë”ë¯¸ urlì„ ë§Œë“¦
    await masure_coroutine_time(asyncio.gather(*reqs))  # run_in_executorë¥¼ ì‚¬ìš©í•  ë•Œ(ê¸°ë³¸ê°’): ë¹ ë¦„(0.11582900000212248)

if __name__ == '__main__':
    asyncio.run(main())
```

#### requests ëª¨ë“ˆê³¼ í˜¸í™˜ë˜ì§€ ì•ŠëŠ” ë¶€ë¶„

ì´ ëª¨ë“ˆì€ `requests` ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ ê±°ì˜ ëª¨ë“  ë¶€ë¶„ì—ì„œ í˜¸í™˜ë˜ì§€ë§Œ í˜¸í™˜ë˜ì§€ ì•ŠëŠ” ë¶€ë¶„ì´ ëª‡ ê°€ì§€ ìˆìŠµë‹ˆë‹¤.

##### dunder method(`__dunder__`)

ì ì •ì  ë²„ê·¸ì˜ ì´ìœ ê°€ ë  ìˆ˜ ìˆë‹¤ëŠ” ì´ìœ  í˜¹ì€ ê¸°ìˆ ì ì¸ ì´ìœ ë¡œ ì¼ë¶€ dunder methodëŠ” ë¶ˆëŸ¬ì™€ì§€ì§€ ì•Šê±°ë‚˜ í˜¸í™˜ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

ì‚¬ìš©í•  ìˆ˜ ì—†ê±°ë‚˜ requests ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ ì¼ì¹˜í•˜ì§€ ì•ŠëŠ” dunder method: `__builtins__`, `__cached__`, `__doc__`, `__file__`, `__loader__`, `__name__`, `__package__`, `__spec__`

ì‚¬ìš© ê°€ëŠ¥í•˜ê³  requests ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ ì¼ì¹˜í•˜ëŠ” dunder method: `__author__`, `__author_email__`, `__build__`, `__cake__`, `__copyright__`, `__description__`, `__license__`, `__title__`, `__url__`, `__version__`

```python
>>> import requests
>>> requests.__name__
'requests'
>>> requests.__path__
['some path']
>>> requests.__cake__
'âœ¨ ğŸ° âœ¨'
>>>
>>> from resoup import requests
>>> requests.__name__  # í˜¸í™˜ë˜ì§€ ì•ŠëŠ” dunder method
'resoup.requests_proxy'  # requestsì™€ ê°’ì´ ë‹¤ë¦„
>>> requests.__path__ # ì‚¬ìš©í•  ìˆ˜ ì—†ê³  í˜¸í™˜ë˜ì§€ ì•ŠëŠ” dunder method
AttributeError: module 'resoup.requests_' has no attribute '__path__'
>>> requests.__cake__  # í˜¸í™˜ë˜ëŠ” dunder method
'âœ¨ ğŸ° âœ¨'
```

##### import

`resoup.requests`ëŠ” ê±°ì˜ ëª¨ë“  ê²½ìš°ì—ì„œ import ê´€ë ¨ í˜¸í™˜ì„±ì´ ìœ ì§€ë©ë‹ˆë‹¤. í•˜ì§€ë§Œ importì™€ ê´€ë ¨í•´ì„œëŠ” ëª‡ ê°€ì§€ ê·œì¹™ì´ ì¡´ì¬í•©ë‹ˆë‹¤.

`resoup.requests`ëŠ” `from resoup import requests`ì˜ í˜•íƒœë¡œë§Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
# ê° ë¼ì¸ì—ì„œ ìœ—ì¤„ê³¼ ì•„ë«ì¤„ì€ ê°ê° requestsë¥¼ import í•  ë•Œì™€ `resoup.requests`ë¥¼ importí•  ë•Œë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

# requests ëª¨ë“ˆ import
import requests
from resoup import requests  # ê°€ëŠ¥
```

ë”°ë¼ì„œ ë‹¤ìŒê³¼ ê°™ì€ ê²½ìš°ëŠ” `resoup.requests`ì—ì„œ importê°€ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.

```python
# requestsì˜ í•˜ìœ„ ëª¨ë“ˆ import
import requests.models  # ê°€ëŠ¥
import resoup.requests.models  # ë¶ˆê°€ëŠ¥!

# requestsì˜ í•˜ìœ„ ëª¨ë“ˆ import (w/ from .. import ...)
from request import models  # ê°€ëŠ¥
from resoup.requests import models  # ë¶ˆê°€ëŠ¥!

# requestsì˜ í•˜ìœ„ ëª¨ë“ˆì˜ í•˜ìœ„ êµ¬ì„± ìš”ì†Œ import
from request.models import Response  # ê°€ëŠ¥
from resoup.requests.models import Response  # ë¶ˆê°€ëŠ¥!
```

ì´ëŸ° ê²½ìš°ì—” ëª¨ë“ˆ importë¥¼ ì´ìš©í•˜ë©´ í•´ê²°ë©ë‹ˆë‹¤..

ì˜ˆë¥¼ ë“¤ì–´ ë‹¤ìŒê³¼ ê°™ì€ ì½”ë“œê°€ ìˆë‹¤ê³  í•´ ë´…ì‹œë‹¤.

```python
from request.models import Response  # í•˜ìœ„ ëª¨ë“ˆì˜ í•˜ìœ„ êµ¬ì„± ìš”ì†Œ import ì‚¬ìš©

def is_response(instance):
    return isinstance(instance, Response)
```

ì´ ì½”ë“œëŠ” ë‹¤ìŒê³¼ ê°™ì´ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
# requests.models.Responseë¡œ ë°”ê¾¸ê¸°.
# ì¥ì : ê¹”ë”í•˜ê³  error-proneí•˜ì§€ ì•ŠìŒ.
from resoup import requests  # requests ëª¨ë“ˆ import
def is_response(instance):
    return isinstance(instance, requests.models.Response)  # requests.models.Responseë¡œ ë³€ê²½í•¨
```

```python
# Response ì •ì˜í•˜ê¸°.
# ì¥ì : ì½”ë“œë¥¼ ìˆ˜ì •í•  í•„ìš”ê°€ ì—†ìŒ.
from resoup import requests
Response = requests.models.Response

def is_response(instance):
    return isinstance(instance, Response)
```

ê°œì¸ì˜ ì„ í˜¸ì— ë”°ë¼ ì›í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì‚¬ìš©í•˜ì‹œë©´ ë©ë‹ˆë‹¤.

### ResponseProxy

`ResponseProxy`ëŠ” ì´ ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ requests.get/options/head/post/put/patch/deleteë¥¼ ì‚¬ìš©í•  ê²½ìš°ì˜ ë¦¬í„´ê°’ì…ë‹ˆë‹¤. ê¸°ì¡´ Responseì™€ 100% í˜¸í™˜ë˜ë©´ì„œë„ ì¶”ê°€ì ì¸ í•¨ìˆ˜ 6ê°œë¥¼ ì œê³µí•©ë‹ˆë‹¤.

#### í˜¸í™˜ì„±

ì´ íŒŒíŠ¸ì—ì„œëŠ” ì£¼ì„ì— ë‚´ìš©ì„ ì ì—ˆìŠµë‹ˆë‹¤.

```python
>>> # ë‘ ëª¨ë“ˆì„ ë™ì‹œì— ì‚¬ìš©í•´ì•¼ í•˜ë‹ˆ ì´ë¦„ì„ ë³€ê²½í•˜ê² ìŠµë‹ˆë‹¤.
>>> import requests as orginal_requests
>>> from resoup import requests as utils_requsts
>>>
>>> # requests ëª¨ë“ˆì€ Responseë¥¼ ì‘ë‹µí•©ë‹ˆë‹¤.
>>> response1 = orginal_requests.get("https://peps.python.org/pep-0020/")  # ì •ì ì¸ ì›¹ì‚¬ì´íŠ¸
>>> print(response1)
<Response [200]>
>>> print(type(response1))  # Response ê°ì²´
<class 'requests.models.Response'>
>>> # resoup.requestsëª¨ë“ˆì€ ResponseProxyë¥¼ ì‘ë‹µí•©ë‹ˆë‹¤.
>>> response2 = utils_requsts.get("https://peps.python.org/pep-0020/")
>>> print(response2)
<Response [200]>
>>> print(type(response2))  # ResponseProxy ê°ì²´
<class 'resoup.response_proxy.ResponseProxy'>
>>>
>>> # ë‹¤ìŒì˜ ëª¨ë“  ê²€ì‚¬ë“¤ì„ í†µê³¼í•©ë‹ˆë‹¤.
>>> assert response1.text == response2.text
>>> assert response1.status_code == response2.status_code
>>> assert response1.url == response2.url
>>> assert response1.content == response2.content
>>>
>>> # í•˜ì§€ë§Œ RequestsProxyì—ëŠ” ì´ëŸ¬í•œ ì¶”ê°€ì ì¸ ê¸°ëŠ¥ë“¤ì´ ì¡´ì¬í•©ë‹ˆë‹¤.
>>> print(response2.soup())
<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
...
<script src="../_static/wrap_tables.js"></script>
<script src="../_static/sticky_banner.js"></script>
</body>
</html>
>>> print(response2.soup_select('title'))
[<title>PEP 20 â€“ The Zen of Python | peps.python.org</title>, <title>Following system colour scheme</title>, <title>Selected dark colour scheme</title>, <title>Selected light colour scheme</title>]
>>> print(response2.soup_select_one('p', no_empty_result=True).text)
Long time Pythoneer Tim Peters succinctly channels the BDFLâ€™s guiding
principles for Pythonâ€™s design into 20 aphorisms, only 19 of which
have been written down.
>>>
>>> from requests.models import Response
>>> # RequestsProxyëŠ” Requsestsì˜ subclassì…ë‹ˆë‹¤.
>>> # ë”°ë¼ì„œ isinstance ê²€ì‚¬ë¥¼ í†µê³¼í•©ë‹ˆë‹¤.
>>> isinstance(response2, Response)
True
>>> # ë¬¼ë¡  subclassì´ê¸° ë•Œë¬¸ì— '==' ê²€ì‚¬ëŠ” í†µê³¼í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
>>> type(response1) == type(response2)
False
```

#### ê¸°ë³¸ êµ¬ì¡°

`ResponseProxy`ì—ëŠ” ì—¬ëŸ¬ ëª¨ë“ˆë“¤ì´ ìˆìœ¼ë©°, í¬ê²Œ ì„¸ ê°€ì§€ ì¢…ë¥˜ë¡œ ë¶„ë¥˜ë©ë‹ˆë‹¤.

* soupë¥˜: `.soup()`, `.soup_select()`, `.soup_select_one()`
  ê¸°ë³¸ì ì¸ í•¨ìˆ˜ì…ë‹ˆë‹¤.
* xmlë¥˜: `.xml()`, `.xml_select()`, `.xml_select_one()`
  soupë¥˜ì—ì„œ parserê°€ 'xml'ì¸ ê²½ìš°ì…ë‹ˆë‹¤.

ê°ê°ì˜ ì¢…ë¥˜ì—ëŠ” ì„¸ ê°€ì§€ í•¨ìˆ˜ê°€ ìˆìœ¼ë©° í•¨ìˆ˜ ê°ê°ì˜ ê¸°ëŠ¥ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

* `.soup()`/`.xml()`: BeatifulSoupë¡œ í•´ì„ëœ ì½”ë“œê°€ ë‚˜ì˜µë‹ˆë‹¤.
* `.soup_select()`/`.xml_select()`: `.soup().select()`ì™€ ë¹„ìŠ·í•©ë‹ˆë‹¤.
* `.soup_select_one()`/`.xml_select_one()`: `.soup().select_one()`ê³¼ ë¹„ìŠ·í•©ë‹ˆë‹¤.

ìì„¸í•œ ë‚´ìš©ì€ ì•„ë˜ë¥¼ ì‚´í´ë³´ì„¸ìš”.

#### `.soup()`

`.soup()`ëŠ” í…ìŠ¤íŠ¸ë‚˜ responseë¥¼ ë°›ì•„ `BeatifulSoup`ë¡œ ë‚´ë³´ëƒ…ë‹ˆë‹¤.

ì´ë•Œ ì¸ìëŠ” responseì™€ response.text ëª¨ë‘ ê°€ëŠ¥í•˜ì§€ë§Œ responseë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ê¶Œí•©ë‹ˆë‹¤.
ê·¸ëŸ¬ë©´ ë”ìš± ìƒì„¸í•œ ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
>>> from resoup import requests
>>>
>>> response = requests.get("https://python.org")
>>> response.soup()  # BeatifulSoupì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  parameter ì‚¬ìš© ê°€ëŠ¥
<!DOCTYPE html>
...
</body>
</html>
```

ì´ í•¨ìˆ˜ëŠ” ì‚¬ì‹¤ìƒ `BeatifulSoup`ë¥¼ í†µê³¼ì‹œí‚¤ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤. ì•„ë˜ì˜ ì½”ë“œëŠ” ìœ„ì˜ ì½”ë“œì™€ ê±°ì˜ ê°™ìŠµë‹ˆë‹¤.

```python
>>> import requests
>>> from bs4 import BeautifulSoup
>>>
>>> response = requests.get("https://python.org")
>>> BeautifulSoup(response.text)
<!DOCTYPE html>
<!DOCTYPE html>
...
</body>
</html>
```

parserê°€ ì—†ì„ ê²½ìš° `BeatifulSoup`ëŠ” `FeatureNotFound`ì—ëŸ¬ê°€ ë‚˜ì˜¤ì§€ë§Œ `.soup()`ëŠ” `NoParserError`ê°€ ë‚˜ì˜µë‹ˆë‹¤.

#### `.soup_select()`

`.soup_select()`ëŠ” í…ìŠ¤íŠ¸ë‚˜ responseë¥¼ ë°›ì•„ BeatifulSoupì˜ Tagë¡œ ë‚´ë³´ëƒ…ë‹ˆë‹¤. `selector` parameterëŠ” CSS ì„ íƒìë¥¼ ë°›ìŠµë‹ˆë‹¤.

```python
>>> from resoup import requests
>>>
>>> response = requests.get("https://python.org")
>>> response.soup_select("p")
[<p><strong>Notice:</strong> While JavaScript is not essential for this website
...]
```

ì•„ë˜ì˜ ì½”ë“œëŠ” ìœ„ì˜ ì½”ë“œì™€ ìœ ì‚¬í•˜ê²Œ ë™ì‘í•©ë‹ˆë‹¤.

```python
>>> import requests
>>> from bs4 import BeautifulSoup
>>>
>>> response = requests.get('https://python.org')
>>> soup = BeautifulSoup(response.text).select('p')
>>> soup
[<p><strong>Notice:</strong> While JavaScript is not essential for this website
...]
```

ì´ í•¨ìˆ˜ì˜ ë…íŠ¹í•œ ì ì€, `no_empty_result`ë¼ëŠ” parameterì˜ ì¡´ì¬ì…ë‹ˆë‹¤. ì´ parameterê°€ Trueì´ë©´ .select()ì˜ ê²°ê³¼ê°€ ë¹ˆ ë¦¬ìŠ¤íŠ¸ì¼ë•Œ `EmptyResultError`ë¥¼ ëƒ…ë‹ˆë‹¤.

```python
>>> from resoup import requests
>>>
>>> response = requests.get("https://python.org")
>>> response.soup_select("data-some-complex-and-error-prone-selector")
[]
>>>
>>> response = requests.get("https://python.org")
>>> response.soup_select(
...     "data-some-complex-and-error-prone-selector",
...     no_empty_result=True)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "...souptools.py", line 148, in soup_select
    raise EmptyResultError(
resoup.exceptions.EmptyResultError: Result of select is empty list("[]"). This error happens probably because of invalid selector or URL. Check if both selector and URL are valid. Set to False `no_empty_result` if empty list is intended. It may also because of selector is not matched with URL.
selector: data-some-complex-and-error-prone-selector, URL: https://www.python.org/
```

ì´ í•¨ìˆ˜ë¥¼ ê¸°ë³¸ì ìœ¼ë¡œ BroadcastListë¥¼ ì¶œë ¥ê°’ìœ¼ë¡œ ì„¤ì •í•˜ê³  ìˆìŠµë‹ˆë‹¤. BroadcastListì— ëŒ€í•´ ìì„¸íˆ ì•Œê³  ì‹¶ë‹¤ë©´ ì•„ë˜ì˜ `BroadcastList` í•­ëª©ì„ í™•ì¸í•´ ë³´ì„¸ìš”.

#### `.soup_select_one()`

`.soup_select_one()`ëŠ” í…ìŠ¤íŠ¸ë‚˜ responseë¥¼ ë°›ì•„ BeatifulSoupì˜ Tagë¡œ ë‚´ë³´ëƒ…ë‹ˆë‹¤. `selector` parameterëŠ” CSS ì„ íƒìë¥¼ ë°›ìŠµë‹ˆë‹¤.

```python
>>> from resoup import requests
>>>
>>> response = requests.get('https://python.org')
>>> response.soup_select_one('p strong', no_empty_result=True)
<strong>Notice:</strong>
```

ì•„ë˜ì˜ ì½”ë“œëŠ” ìœ„ì˜ ì½”ë“œì™€ ìœ ì‚¬í•˜ê²Œ ë™ì‘í•©ë‹ˆë‹¤.

```python
>>> import requests
>>> from bs4 import BeautifulSoup
>>>
>>> response = requests.get('https://python.org')
>>> soup = BeautifulSoup(response.text, 'html.parser').select('p strong')
>>> if soup is None:  # no_empty_result ê´€ë ¨ í™•ì¸ ì½”ë“œ
...     raise Exception
...
>>> soup
<strong>Notice:</strong>
```

`no_empty_result` parameterê°€ Trueì´ë©´ .select_one()ì˜ ê²°ê³¼ê°€ Noneì¼ë•Œ `EmptyResultError`ë¥¼ ëƒ…ë‹ˆë‹¤.

ì´ ê¸°ëŠ¥ì€ íƒ€ì… íŒíŠ¸ì—ì„œë„ ìœ ìš©í•˜ê²Œ ì“°ì¼ ìˆ˜ ìˆê³ , ì˜¤ë¥˜ë¥¼ ë” ëª…í™•íˆ í•˜ëŠ” ë°ì—ë„ ë„ì›€ì„ ì¤ë‹ˆë‹¤.

ê¸°ì¡´ BeatifulSoupì—ì„œëŠ” `.select_one()`ì˜ ë¦¬í„´ê°’ì„ `Tag | None`ìœ¼ë¡œ í‘œì‹œí–ˆê¸° ë•Œë¬¸ì— ë§Œì•½ `.select_one().text`ì™€ ê°™ì€ ì½”ë“œë¥¼ ì‚¬ìš©í•˜ë ¤ê³  í•˜ë©´ ì •ì  íƒ€ì… ê²€ì‚¬ ë„êµ¬ë“¤ì—ì„œ ì˜¤ë¥˜ë¥¼ ë°œìƒì‹œì¼°ìŠµë‹ˆë‹¤.

íŠ¹íˆ `.select_one()`ì˜ ê²°ê³¼ê°€ Noneì´ ë˜ë©´ `'NoneType' object has no attribute 'text'`ë¼ëŠ” ì–´ë–¤ ë¶€ë¶„ì—ì„œ ì˜¤ë¥˜ê°€ ë‚¬ëŠ”ì§€ í•œëˆˆì— í™•ì¸í•˜ê¸° í˜ë“  ì˜¤ë¥˜ ë©”ì‹œì§€ê°€ ë‚˜ì™”ìŠµë‹ˆë‹¤.

`no_empty_result`ë¥¼ ì´ìš©í•˜ë©´ ì´ëŸ¬í•œ ë¬¸ì œë“¤ì„ í•´ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
`no_empty_result`ë¥¼ Trueë¡œ í•˜ë©´ íƒ€ì… ê²€ì‚¬ ë„êµ¬ë“¤ë„ ì¡°ìš©í•´ì§€ê³ , í˜¹ì‹œë¼ë„ Noneì´ ê²°ê³¼ê°’ì´ ë  ë•Œ  ëŒ€ì‹  í›¨ì”¬ ë” ìì„¸í•˜ë©° í•´ê²°ì±…ì„ í¬í•¨í•œ ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ ë§Œë“¤ì–´ ëƒ…ë‹ˆë‹¤.

```python
>>> from resoup import requests
>>>
>>> response = requests.get("https://python.org")
>>> print(response.soup_select_one("data-some-complex-and-error-prone-selector"))
None  # ì‹¤ì œë¡œ Noneì´ ê²°ê³¼ê°’ìœ¼ë¡œ ë‚˜ì˜¤ì§„ ì•Šê³  ê·¸ëƒ¥ ì¡°ìš©íˆ ì¢…ë£Œë¨.
>>>
>>> response = requests.get("https://python.org")
>>> response.soup_select_one(
...     "data-some-complex-and-error-prone-selector",
...     no_empty_result=True)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "...souptools.py", line 220, in soup_select_one
    raise EmptyResultError(
resoup.exceptions.EmptyResultError: Result of select_one is None. This error happens probably because of invalid selector or URL. Check if both selector and URL are valid. Set to False `no_empty_result` if empty list is intended. It may also because of selector is not matched with URL.  
selector: data-some-complex-and-error-prone-selector, URL: https://www.python.org/
```

#### xml ê´€ë ¨ í•¨ìˆ˜

`ResponseProxy`ì˜ `soup` ê´€ë ¨ í•¨ìˆ˜ì—ì„œ `soup`ë¥¼ `xml`ë¡œ ì¹˜í™˜í•˜ë©´ xml í•¨ìˆ˜ê°€ ë©ë‹ˆë‹¤.

ì´ í•¨ìˆ˜ë“¤ì€ parserê°€ `'xml'`ì´ë¼ëŠ” ì ì„ ì œì™¸í•˜ê³ ëŠ” soupì™€ ì°¨ì´ì ì´ ì—†ìŠµë‹ˆë‹¤.

ì˜ˆì‹œ ì½”ë“œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤

```python
>>> from resoup import requests
>>>
>>> response = requests.get('https://www.w3schools.com/xml/plant_catalog.xml')
>>> selected = response.xml_select('LIGHT', no_empty_result=True)
>>> selected
[<LIGHT>Mostly Shady</LIGHT>, <LIGHT>Mostly Shady</LIGHT>, <LIGHT>Mostly Sunny</LIGHT>, <LIGHT>Mostly Shady</LIGHT>, <LIGHT>Mostly Shady</LIGHT>, <LIGHT>Mostly Shady</LIGHT>, <LIGHT>Mostly Shady</LIGHT>, <LIGHT>Mostly Shady</LIGHT>, <LIGHT>Mostly Shady</LIGHT>, <LIGHT>Mostly Shady</LIGHT>, <LIGHT>Sun or Shade</LIGHT>, <LIGHT>Sun or Shade</LIGHT>, <LIGHT>Mostly Shady</LIGHT>, <LIGHT>Sun or Shade</LIGHT>, <LIGHT>Sun or Shade</LIGHT>, <LIGHT>Shade</LIGHT>, <LIGHT>Shade</LIGHT>, <LIGHT>Shade</LIGHT>, <LIGHT>Mostly Shady</LIGHT>, <LIGHT>Mostly Shady</LIGHT>, <LIGHT>Shade</LIGHT>, <LIGHT>Shade</LIGHT>, <LIGHT>Sunny</LIGHT>, <LIGHT>Shade</LIGHT>, <LIGHT>Shade</LIGHT>, <LIGHT>Sunny</LIGHT>, <LIGHT>Shade</LIGHT>, <LIGHT>Sunny</LIGHT>, <LIGHT>Sun or Shade</LIGHT>, <LIGHT>Sun or Shade</LIGHT>, <LIGHT>Shade</LIGHT>, <LIGHT>Shade</LIGHT>, <LIGHT>Sun</LIGHT>, <LIGHT>Mostly Shady</LIGHT>, <LIGHT>Shade</LIGHT>, <LIGHT>Shade</LIGHT>]
```

ìœ„ì˜ ì½”ë“œëŠ” ì•„ë˜ì˜ ì½”ë“œì™€ ê±°ì˜ ê°™ìŠµë‹ˆë‹¤.

```python
>>> from resoup import requests
>>> from functools import partial
>>>
>>> response = requests.get('https://www.w3schools.com/xml/plant_catalog.xml')
>>> # corespond to `.xml_select()`
>>> xml_select_partial = partial(response.soup_select, parser='xml')
>>> selected = xml_select_partial('LIGHT', no_empty_result=True)
>>> selected
[<LIGHT>Mostly Shady</LIGHT>, <LIGHT>Mostly Shady</LIGHT>, <LIGHT>Mostly Sunny</LIGHT>, <LIGHT>Mostly Shady</LIGHT>, <LIGHT>Mostly Shady</LIGHT>, <LIGHT>Mostly Shady</LIGHT>, <LIGHT>Mostly Shady</LIGHT>, <LIGHT>Mostly Shady</LIGHT>, <LIGHT>Mostly Shady</LIGHT>, <LIGHT>Mostly Shady</LIGHT>, <LIGHT>Sun or Shade</LIGHT>, <LIGHT>Sun or Shade</LIGHT>, <LIGHT>Mostly Shady</LIGHT>, <LIGHT>Sun or Shade</LIGHT>, <LIGHT>Sun or Shade</LIGHT>, <LIGHT>Shade</LIGHT>, <LIGHT>Shade</LIGHT>, <LIGHT>Shade</LIGHT>, <LIGHT>Mostly Shady</LIGHT>, <LIGHT>Mostly Shady</LIGHT>, <LIGHT>Shade</LIGHT>, <LIGHT>Shade</LIGHT>, <LIGHT>Sunny</LIGHT>, <LIGHT>Shade</LIGHT>, <LIGHT>Shade</LIGHT>, <LIGHT>Sunny</LIGHT>, <LIGHT>Shade</LIGHT>, <LIGHT>Sunny</LIGHT>, <LIGHT>Sun or Shade</LIGHT>, <LIGHT>Sun or Shade</LIGHT>, <LIGHT>Shade</LIGHT>, <LIGHT>Shade</LIGHT>, <LIGHT>Sun</LIGHT>, <LIGHT>Mostly Shady</LIGHT>, <LIGHT>Shade</LIGHT>, <LIGHT>Shade</LIGHT>]
```

#### BroadcastList

`.soup_select()`ì™€ `.xml_select()`ì˜ ê²½ìš°ì—ëŠ” ë¦¬ìŠ¤íŠ¸ë¥¼ ê°’ìœ¼ë¡œ ë‚´ë³´ëƒ…ë‹ˆë‹¤. ì´ëŠ” `.soup()`ë‚˜ `.soup_select_one()`ì—ì„œ ê¸°ëŒ€í•  ìˆ˜ ìˆëŠ” `.text`ì™€ ê°™ì€ íŒŒë¼ë¯¸í„° ì‚¬ìš©ì„ ì–´ë µê²Œ í•©ë‹ˆë‹¤.

ì´ëŠ” for loopë‚˜ ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ìœ¼ë¡œ í•´ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
>>> from resoup import requests
>>> tags_list = requests.get("https://python.org").soup_select("p strong")
>>> [element.text for element in tags_list]
['Notice:', 'relaunched community-run job board']
```

í•˜ì§€ë§Œ ì´ê²ƒì´ ë§ˆìŒì— ë“¤ì§€ ì•Šì„ ìˆ˜ê°€ ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ ê°œë°œ ì¤‘ì´ë¼ë©´ ë¹ ë¥¸ _ê°œë°œ_ ì†ë„ë¥¼ ìœ„í•´ for loopë‚˜ ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒ ì™¸ì— ë” ì‹ ì†í•˜ê²Œ `.text` ë“±ì„ ì ìš©í•˜ëŠ” ë°©ë²•ì„ ê³ ë ¤í•˜ê³  ì‹¶ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì´ í”„ë¡œì íŠ¸ì˜ `.soup_select()`ì˜ ê¸°ë³¸ ë¦¬í„´ê°’ìœ¼ë¡œ ì„¤ì •ëœ BroadcastListëŠ” ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ë°©í¸ì…ë‹ˆë‹¤.

BroadcastListì—ì„œëŠ” ë¦¬ìŠ¤íŠ¸ë¥¼ í†µí•´ ì§ì ‘ Tagì—ì„œ ì‚¬ìš©ë˜ëŠ” ì†ì„±ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
>>> from resoup import requests
>>> tags_list = requests.get("https://python.org").soup_select("p strong")
>>> tags_list
[<strong>Notice:</strong>, <strong>relaunched community-run job board</strong>]
>>> type(tags_list)
<class 'resoup.broadcast_list.TagBroadcastList'>  # BroadcastListê°€ ì‚¬ìš©ë¨
>>> tags_list.text  # ë¸Œë¡œë“œìºìŠ¤íŒ…
['Notice:', 'relaunched community-run job board']
>>>
>>> tags_list_with_no_broadcast_list = requests.get('https://python.org').soup_select('p', use_broadcast_list=False)
>>> type(tags_list_with_no_broadcast_list)
<class 'bs4.element.ResultSet'>  # BroadcastListê°€ ì‚¬ìš©ë˜ì§€ ì•ŠìŒ
>>> tags_list_with_no_broadcast_list.text
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "...element.py", line 2428, in __getattr__
    raise AttributeError(
AttributeError: ResultSet object has no attribute 'text'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?
```

BroadcastListëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë°©ë²•ì„ í†µí•´ ëŒ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
>>> from resoup import requests
>>>
>>> tags_list = requests.get("https://python.org").soup_select("p", use_broadcase_list=False)
>>> type(tags_list)
bs4.element.ResultSet
>>> tags_list.text  # ë¸Œë¡œë“œìºìŠ¤íŒ… ì•ˆ ë¨
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "...element.py", line 2428, in __getattr__
    raise AttributeError(
AttributeError: ResultSet object has no attribute 'text'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?
```

### íŠ¹ë³„í•œ í˜•íƒœì˜ ë¦¬ìŠ¤íŠ¸ getitem

BroadCastListì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì€ íŠ¹ì´í•œ ê¸°ëŠ¥ì´ ìˆìŠµë‹ˆë‹¤.

ë§Œì•½ ë¦¬ìŠ¤íŠ¸ì— ì •ìˆ˜ë‚˜ ìŠ¬ë¼ì´ìŠ¤ë¡œ getitemì„ ìš”ì²­í•œë‹¤ë©´ ì¼ë°˜ì ì¸ ë¦¬ìŠ¤íŠ¸ì˜ ì—­í• ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

```python
>>> from resoup import requests
>>> # ê°’ ë¶ˆëŸ¬ì˜´()
>>> tag_broadcast_list = requests.cget("https://www.python.org/community/logos/").soup_select("img")
>>> tag_broadcast_list
[<img alt="Python Software Foundation" class="psf-logo" src="/static/img/psf-logo.png"/>,
...
<img alt="Logo device only" src="https://s3.dualstack.us-east-2.amazonaws.com/pythondotorg-assets/media/community/logos/python-logo-only.png" style="height: 48px;"/>,
<img alt="/static/community_logos/python-powered-w-100x40.png" src="/static/community_logos/python-powered-w-100x40.png"/>,
<img alt="/static/community_logos/python-powered-h-50x65.png" src="/static/community_logos/python-powered-h-50x65.png"/>]
>>> # ì •ìˆ˜ getitem
>>> tag_broadcast_list[0]
<img alt="Python Software Foundation" class="psf-logo" src="/static/img/psf-logo.png"/>
>>> # ìŠ¬ë¼ì´ì‹±
>>> tag_broadcast_list[3:5]
[<img alt="/static/community_logos/python-powered-w-100x40.png" src="/static/community_logos/python-powered-w-100x40.png"/>,
 <img alt="/static/community_logos/python-powered-h-50x65.png" src="/static/community_logos/python-powered-h-50x65.png"/>]
>>> # ë¬¸ìì—´ getitem (ë¸Œë¡œë“œìºìŠ¤íŒ… ì ìš©ë¨!)
>>> tag_broadcast_list["alt"]
['Python Software Foundation',
 'Combined logo',
 'Logo device only',
 '/static/community_logos/python-powered-w-100x40.png',
 '/static/community_logos/python-powered-h-50x65.png']
```

### CustomDefaults

`CustomDefaults`ë¥¼ í†µí•´ ì§ì ‘ ê¸°ë³¸ê°’ì„ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ê°’ìœ¼ë¡œ ì¼ë°˜ get/options/head/post/put/patch/delete ë° c../a../ac.. í•¨ìˆ˜ì˜ ê¸°ë³¸ê°’ì„ íš¨ê³¼ì ìœ¼ë¡œ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
>>> from resoup import CustomDefaults
>>>
>>> requests = CustomDefaults(headers={'User-Agent': 'User Agent for Test'})
>>> requests.get('https://httpbin.org/headers').json()['headers']['User-Agent']
'User Agent for Test'
```

## ë¼ì´ì„ ìŠ¤ ì •ë³´

ì´ í”„ë¡œê·¸ë¨ì€ MIT ë¼ì´ì„ ìŠ¤ë¡œ ê³µìœ ë©ë‹ˆë‹¤.

ì´ í”„ë¡œê·¸ë¨ì˜ ì¼ë¶€ëŠ” [requests(Apache License 2.0)](https://github.com/psf/requests) ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ìˆë˜ ì½”ë“œë¥¼ í¬í•¨í•©ë‹ˆë‹¤.
Some part of this program contains code from [requests](https://github.com/psf/requests) library.

ì´ í”„ë¡œê·¸ë¨ì˜ ì¼ë¶€ëŠ” [typeshed(Apache License 2.0 or MIT License)](https://github.com/python/typeshed) ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ìˆë˜ ì½”ë“œë¥¼ í¬í•¨í•©ë‹ˆë‹¤.
Some part of this program contains code from [typeshed](https://github.com/python/typeshed) library.

## Relese Note

0.5.2 (2023-12-26): Timeout ì˜¤ë¥˜ë„ attemptsì— ê±¸ë¦´ ìˆ˜ ìˆë„ë¡ ë³€ê²½, rootì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë³€ìˆ˜ ì¶”ê°€, ë¹Œë“œ ì½”ë“œ ê°œì„ , ì½”ë“œ ê°œì„ 

0.5.1 (2023-12-9): ë²„ê·¸ ìˆ˜ì •

0.5.0 (2023-12-9): resoupë¡œ ì´ë¦„ ë³€ê²½, ìƒˆ BroadcastList ê¸°ë³¸ ì ìš©, poetry ì‚¬ìš©, ê¸°ì¡´ souptools ëª¨ë“ˆ ì œê±° ë° souptoolsclass ëª¨ë“ˆë¡œ ëŒ€ì²´, í…ŒìŠ¤íŠ¸ ì¶”ê°€

0.4.1 (2023-11-4): ê¸´ê¸‰ ë²„ê·¸ ìˆ˜ì •

0.4.0 (2023-11-4): raise_for_status ê¸°ë³¸ê°’ ë³€ê²½, souptoolsclass ì¶”ê°€, avoid_sslerror ì¶”ê°€

0.3.0 (2023-10-05): BroadcastList ë³µì›, sessions_with_tools ì¶”ê°€

0.2.3 (2023-09-19): header ê¸°ë³¸ê°’ ë³€ê²½, ConnectionErrorì‹œ ì—ëŸ¬ í•œ ê°œë§Œ ë³´ì´ëŠ” ê²ƒìœ¼ë¡œ ë³€ê²½, attemptsë¡œ ì¬ì‹œë„í•  ë•Œ ì„±ê³µí–ˆì„ ë•Œ ë©”ì‹œì§€ ì¶”ê°€, retryì—ì„œ url ì œê±°, setup.pyì™€ ê´€ë ¨ íŒŒì¼ ë³€ê²½

0.2.2 (2023-09-08): attempt parameterë¥¼ attemptsë¡œ ë³€ê²½, BroadcastList ì œê±°

0.2.1 (2023-08-31): py.typed ì¶”ê°€, freeze_dict_and_list ì¶”ê°€

0.2.0 (2023-08-27): CustomDefaults ì¶”ê°€

0.1.1 (2023-08-27): ì²« ë¦´ë¦¬ì¦ˆ

