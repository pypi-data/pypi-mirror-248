# @package _global_

# ---------------------------------------------------------------------------- #
#                         任务标识、描述、Comet 设置等                                      
# ---------------------------------------------------------------------------- #
memo: 【2dist_loss】vicuna 在 faithdial 数据集上的训练，使用默认的 prompt

# 是否开启 Comet.ml 实验记录
start_comet_log: True
comet_project_name: Personal
comet_exp_name: vicuna-faithdial-default-dist_loss
stage: train  # debug、train、finetune、test
comet_tags:
  - dist_loss
  - vicuna
  - faithdial
  - prompt_1


# ---------------------------------------------------------------------------- #
#                               数据、模型加载                               
# ---------------------------------------------------------------------------- #
model_name_or_path: lmsys/vicuna-7b-v1.5
# model_name_or_path: /home/dengyf/projects/outputs/Personal_2023121622195458583533723702121089829/best
train_data_file: ${root_dir}/data/faithdial/train.json
val_data_file: ${root_dir}/data/faithdial/valid_random_split.json
test_data_file: ${root_dir}/data/faithdial/test_random_split.json
dataset_module_file: projects.modules.faith_dial.data_processor.FaithDialProcessor
lit_model_file: projects.modules.faith_dial.standard.LitModel

train_batch_size: 4
val_batch_size: 4
test_batch_size: 16


# ---------------------------------------------------------------------------- #
#                                  训练参数                                     
# ---------------------------------------------------------------------------- #
max_epochs: 1
# fast_dev_run: 1
lr: 5e-4
max_uer_input_length: 1024
max_target_length: 1024

lr_scheduler_type: linear
warmup_steps: 100

use_qlora: True
# use_qlora: False
lora_rank: 256
lora_alpha: 512
lora_dropout: 0.05
merge_lora: True  # 是否在训练完成后合并权重

optim: paged_adamw_32bit
gradient_checkpointing: true
fp16: true
weight_decay: 0



eval_metrics:
  - sacrebleu_sent
  - sacrebleu_corpus
  - dist

# ---------------------------------------------------------------------------- #
#                         Lightning Trainer Args                                     
# ---------------------------------------------------------------------------- #
lit_args:
  # limit_train_batches: 100
  gradient_clip_val: 0.3


