{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f0d70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(x, w=1., phase=0, gain=1):\n",
    "   return gain / (1. + np.exp(-w * (x - phase)))\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set_style()\n",
    "xy = pd.DataFrame(np.arange(-50, 50) / 10., columns=['x'])\n",
    "for w, phase in zip([1, 3, 1, 1, .5], [0, 0, 2, -1, 0]):\n",
    "   kwargs = dict(w=w, phase=phase)\n",
    "   xy[f'{kwargs}'] = logistic(xy['x'], **kwargs)\n",
    "xy.plot(grid=\"on\", ylabel=\"y\")\n",
    "from collections import Counter\n",
    "np.random.seed(451)\n",
    "tokens = \"green egg egg ham ham ham spam spam spam spam\".split()\n",
    "bow = Counter(tokens)\n",
    "x = pd.Series(bow)\n",
    "x\n",
    "x1, x2, x3, x4 = x\n",
    "x1, x2, x3, x4\n",
    "w0 = np.round(.1 * np.random.randn(), 2)\n",
    "w0\n",
    "w1, w2, w3, w4 = (.1 * np.random.randn(len(x))).round(2)\n",
    "w1, w2, w3, w4\n",
    "x = np.array([1, x1, x2, x3, x4])  # <1>\n",
    "w = np.array([w0, w1, w2, w3, w4])  # <2>\n",
    "y = np.sum(w * x)  # <3>\n",
    "y\n",
    "threshold = 0.0\n",
    "y = int(y > threshold)\n",
    "y = logistic(x)\n",
    "def neuron(x, w):\n",
    "   z = sum(wi * xi for xi, wi in zip(x, w))  # <1>\n",
    "   return z > 0  # <2>\n",
    "def neuron(x, w):\n",
    "   z = np.array(wi).dot(w)\n",
    "   return z > 0\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.options.display.max_rows = 7\n",
    "np.random.seed(451)\n",
    "df = pd.read_csv(  # <1>\n",
    "    'https://proai.org/baby-names-us.csv.gz')\n",
    "df.to_csv(  # <2>\n",
    "    'baby-names-us.csv.gz', compression='gzip')\n",
    "df = df.sample(10_000)  # <3>\n",
    "df.shape\n",
    "df.groupby(['name', 'sex'])['count'].sum()[('Timothy',)]\n",
    "df = df.set_index(['name', 'sex'])\n",
    "groups = df.groupby(['name', 'sex'])\n",
    "counts = groups['count'].sum()\n",
    "counts\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    use_idf=False,  # <1>\n",
    "    analyzer='char',\n",
    "    ngram_range=(1, 3)  # <2>\n",
    "    )\n",
    "vectorizer\n",
    "df = pd.DataFrame([list(tup) for tup in counts.index.values],\n",
    "                  columns=['name', 'sex'])\n",
    "df['count'] = counts.values\n",
    "df\n",
    "df['istrain'] = np.random.rand(len(df)) < .9\n",
    "df\n",
    "df.index = pd.MultiIndex.from_tuples(\n",
    "    zip(df['name'], df['sex']), names=['name_', 'sex_'])\n",
    "df\n",
    "df_most_common = {}  # <1>\n",
    "for name, group in df.groupby('name'):\n",
    "    row_dict = group.iloc[group['count'].argmax()].to_dict()  # <2>\n",
    "    df_most_common[(name, row_dict['sex'])] = row_dict\n",
    "df_most_common = pd.DataFrame(df_most_common).T  # <3>\n",
    "df_most_common['istest'] = ~df_most_common['istrain'].astype(bool)\n",
    "df_most_common\n",
    "df['istest'] = df_most_common['istest']\n",
    "df['istest'] = df['istest'].fillna(False)\n",
    "df['istrain'] = ~df['istest']\n",
    "istrain = df['istrain']\n",
    "df['istrain'].sum() / len(df)\n",
    "df['istest'].sum() / len(df)\n",
    "(df['istrain'].sum() + df['istest'].sum()) / len(df)\n",
    "unique_names = df['name'][istrain].unique()\n",
    "unique_names = df['name'][istrain].unique()\n",
    "vectorizer.fit(unique_names)\n",
    "vecs = vectorizer.transform(df['name'])\n",
    "vecs\n",
    "vecs = pd.DataFrame(vecs.toarray())\n",
    "vecs.columns = vectorizer.get_feature_names_out()\n",
    "vecs.index = df.index\n",
    "vecs.iloc[:,:7]\n",
    "vectorizer = TfidfVectorizer(analyzer='char',\n",
    "   ngram_range=(1, 3), use_idf=False, lowercase=False)\n",
    "vectorizer = vectorizer.fit(unique_names)\n",
    "vecs = vectorizer.transform(df['name'])\n",
    "vecs = pd.DataFrame(vecs.toarray())\n",
    "vecs.columns = vectorizer.get_feature_names_out()\n",
    "vecs.index = df.index\n",
    "vecs.iloc[:,:5]\n",
    "import pandas as pd\n",
    "import re\n",
    "dfs = pd.read_html('https://en.wikipedia.org/wiki/'\n",
    "    + 'Comparison_of_deep-learning_software')\n",
    "tabl = dfs[0]\n",
    "bincols = list(tabl.loc[:, 'OpenMP support':].columns)\n",
    "bincols += ['Open source', 'Platform', 'Interface']\n",
    "dfd = {}\n",
    "for i, row in tabl.iterrows():\n",
    "   rowd = row.fillna('No').to_dict()\n",
    "   for c in bincols:\n",
    "       text = str(rowd[c]).strip().lower()\n",
    "       tokens = re.split(r'\\W+', text)\n",
    "       tokens += '\\*'\n",
    "       rowd[c] = 0\n",
    "       for kw, score in zip(\n",
    "               'yes via roadmap no linux android python \\*'.split(),\n",
    "               [1, .9, .2, 0, 2, 2, 2, .1]):\n",
    "           if kw in tokens:\n",
    "               rowd[c] = score\n",
    "               break\n",
    "   dfd[i] = rowd\n",
    "tabl = pd.DataFrame(dfd).T\n",
    "scores = tabl[bincols].T.sum()  # <1>\n",
    "tabl['Portability'] = scores\n",
    "tabl = tabl.sort_values('Portability', ascending=False)\n",
    "tabl = tabl.reset_index()\n",
    "tabl[['Software', 'Portability']][:10]\n",
    "import torch\n",
    "class LogisticRegressionNN(torch.nn.Module):\n",
    "model = LogisticRegressionNN(num_features=vecs.shape[1], num_outputs=1)\n",
    "model\n",
    "loss_func_train = torch.nn.BCELoss(\n",
    "    weight=torch.Tensor(df[['count']][istrain].values))\n",
    "loss_func_test = torch.nn.BCELoss(  # <1>\n",
    "    weight=torch.Tensor(df[['count']][~istrain].values))\n",
    "loss_func_train\n",
    "from torch.optim import SGD\n",
    "hyperparams = {'momentum': 0.001, 'lr': 0.02}  # <1>\n",
    "optimizer = SGD(\n",
    "    model.parameters(), **hyperparams)  # <2>\n",
    "optimizer\n",
    "X = vecs.values\n",
    "y = (df[['sex']] == 'F').values\n",
    "X_train = torch.Tensor(X[istrain])\n",
    "X_test = torch.Tensor(X[~istrain])\n",
    "y_train = torch.Tensor(y[istrain])\n",
    "y_test = torch.Tensor(y[~istrain])\n",
    "from tqdm import tqdm\n",
    "num_epochs = 200\n",
    "pbar_epochs = tqdm(range(num_epochs), desc='Epoch:', total=num_epochs)\n",
    "for epoch in pbar_epochs:\n",
    "     optimizer.zero_grad()  # <1>\n",
    "     outputs = model(X_train)\n",
    "     loss_train = loss_func_train(outputs, y_train)  # <2>\n",
    "     loss_train.backward()  # <3>\n",
    "     optimizer.step()  # <4>\n",
    "def make_array(x):\n",
    "    if hasattr(x, 'detach'):\n",
    "        return torch.squeeze(x).detach().numpy()\n",
    "    return x\n",
    "def measure_binary_accuracy(y_pred, y):\n",
    "    y_pred = make_array(y_pred).round()\n",
    "    y = make_array(y).round()\n",
    "    num_correct = (y_pred == y).sum()\n",
    "    return num_correct / len(y)\n",
    "X = vectorizer.transform(\n",
    "    ['John', 'Greg', 'Vishvesh',  # <1>\n",
    "model(torch.Tensor(X.todense()))\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
