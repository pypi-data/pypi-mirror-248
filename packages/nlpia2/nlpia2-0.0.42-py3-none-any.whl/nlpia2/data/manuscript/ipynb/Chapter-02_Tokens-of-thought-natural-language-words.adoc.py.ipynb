{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a88d0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (\"Trust me, though, the words were on their way, and when \"\n",
    "        \"they arrived, Liesel would hold them in her hands like \"\n",
    "        \"the clouds, and she would wring them out, like the rain.\")\n",
    "tokens = text.split()\n",
    "tokens[:8]\n",
    "import re\n",
    "pattern = r'\\w+(?:\\'\\w+)?|[^\\w\\s]'  # <1>\n",
    "texts = [text]\n",
    "texts.append(\"There's no such thing as survival of the fittest. \"\n",
    "             \"Survival of the most adequate, maybe.\")\n",
    "tokens = list(re.findall(pattern, texts[-1]))\n",
    "tokens[:8]\n",
    "tokens[8:16]\n",
    "tokens[16:]\n",
    "import numpy as np  # <1>\n",
    "vocab = sorted(set(tokens))  # <2>\n",
    "' '.join(vocab[:12])  # <3>\n",
    "num_tokens = len(tokens)\n",
    "num_tokens\n",
    "vocab_size = len(vocab)\n",
    "vocab_size\n",
    "import pandas as pd\n",
    "onehot_vectors = np.zeros(\n",
    "    (len(tokens), vocab_size), int)  # <1>\n",
    "for i, tok in enumerate(tokens):\n",
    "    if tok not in vocab:\n",
    "        continue\n",
    "    onehot_vectors[i, vocab.index(tok)] = 1  # <2>\n",
    "df_onehot = pd.DataFrame(onehot_vectors, columns=vocab)\n",
    "df_onehot.shape\n",
    "df_onehot.iloc[:,:8].replace(0, '')  # <3>\n",
    "import spacy  # <1>\n",
    "from nlpia2.spacy_language_model import load  # <2>\n",
    "nlp = load('en_core_web_sm')  # <3>\n",
    "nlp\n",
    "doc = nlp(texts[-1])\n",
    "type(doc)\n",
    "tokens = [tok.text for tok in doc]  # <1>\n",
    "tokens[:9]  # <2>\n",
    "tokens[9:17]\n",
    "from spacy import displacy\n",
    "sentence = list(doc.sents)[0] # <1>\n",
    "displacy.serve(sentence, style=\"dep\")\n",
    "!firefox 127.0.0.1:5000\n",
    "import requests\n",
    "text = requests.get('https://proai.org/nlpia2-ch2.adoc').text\n",
    "f'{round(len(text) / 10_000)}0k'  # <1>\n",
    "from nlpia2.spacy_language_model import load\n",
    "nlp = load('en_core_web_sm')\n",
    "%timeit nlp(text)  # <1>\n",
    "f'{round(len(text) / 10_000)}0k'\n",
    "doc = nlp(text)\n",
    "f'{round(len(list(doc)) / 10_000)}0k'\n",
    "f'{round(len(doc) / 1_000 / 4.67)}kWPS'  # <2>\n",
    "nlp.pipe_names  # <1>\n",
    "nlp = load('en_core_web_sm', disable=['tok2vec', 'tagger', 'parser'])\n",
    "nlp.pipe_names\n",
    "%timeit nlp(text)\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "%timeit word_tokenize(text)\n",
    "tokens = word_tokenize(text)\n",
    "f'{round(len(tokens) / 10_000)}0k'\n",
    "pattern = r'\\w+(?:\\'\\w+)?|[^\\w\\s]'\n",
    "tokens = re.findall(pattern, text)  # <1>\n",
    "f'{round(len(tokens) / 10_000)}0k'\n",
    "%timeit re.findall(pattern, text)\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2), analyzer='char')\n",
    "vectorizer.fit(texts)\n",
    "bpevocab_list = [\n",
    "   sorted((i, s) for s, i in vectorizer.vocabulary_.items())]\n",
    "bpevocab_dict = dict(bpevocab_list[0])\n",
    "list(bpevocab_dict.values())[:7]\n",
    "vectors = vectorizer.transform(texts)\n",
    "df = pd.DataFrame(\n",
    "    vectors.todense(),\n",
    "    columns=vectorizer.vocabulary_)\n",
    "df.index = [t[:8] + '...' for t in texts]\n",
    "df = df.T\n",
    "df['total'] = df.T.sum()\n",
    "df\n",
    "df.sort_values('total').tail(3)\n",
    "df['n'] = [len(tok) for tok in vectorizer.vocabulary_]\n",
    "df[df['n'] > 1].sort_values('total').tail()\n",
    "hi_text = 'Hiking home now'\n",
    "hi_text.startswith('Hi')\n",
    "pattern = r'\\w+(?:\\'\\w+)?|[^\\w\\s]'  # <1>\n",
    "'Hi' in re.findall(pattern, hi_text)  # <2>\n",
    "'Hi' == re.findall(pattern, hi_text)[0]  # <3>\n",
    "bow = sorted(set(re.findall(pattern, text)))\n",
    "bow[:9]\n",
    "bow[9:19]\n",
    "bow[19:27]\n",
    "v1 = pd.np.array([1, 2, 3])\n",
    "v2 = pd.np.array([2, 3, 4])\n",
    "v1.dot(v2)\n",
    "(v1 * v2).sum()  # <1>\n",
    "sum([x1 * x2 for x1, x2 in zip(v1, v2)])  # <2>\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "texts.append(\n",
    "  \"If conscience and empathy were impediments to the advancement of \"\n",
    "  \"self-interest, then we would have evolved to be amoral sociopaths.\"\n",
    "  )  # <1>\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(texts[-1])[:6]\n",
    "tokens[:8]\n",
    "tokens[8:16]\n",
    "tokens[16:]\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"Nice guys finish first.\"  # <1>\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(f\"{token.text:<11}{token.pos_:<10}{token.dep:<10}\")\n",
    "seg_list = jieba.cut(\"西安是一座举世闻名的文化古城\")  # <1>\n",
    "list(seg_list)\n",
    "import jieba\n",
    "seg_list = jieba.cut(\"西安是一座举世闻名的文化古城\", cut_all=True)  # <1>\n",
    "list(seg_list)\n",
    "seg_list = jieba.cut_for_search(\"西安是一座举世闻名的文化古城\")  # <1>\n",
    "list(seg_list)\n",
    "import jieba\n",
    "from jieba import posseg\n",
    "words = posseg.cut(\"西安是一座举世闻名的文化古城\")\n",
    "jieba.enable_paddle()  # <1>\n",
    "words = posseg.cut(\"西安是一座举世闻名的文化古城\", use_paddle=True)\n",
    "list(words)\n",
    "import spacy\n",
    "spacy.cli.download(\"zh_core_web_sm\")  # <1>\n",
    "nlpzh = spacy.load(\"zh_core_web_sm\")\n",
    "doc = nlpzh(\"西安是一座举世闻名的文化古城\")\n",
    "[(tok.text, tok.pos_) for tok in doc]\n",
    "from nltk.tokenize.casual import casual_tokenize\n",
    "texts.append(\"@rickrau mind BLOOOOOOOOWWWWWN by latest lex :*) !!!!!!!!\")\n",
    "casual_tokenize(texts[-1], reduce_len=True)\n",
    "import requests\n",
    "url = (\"https://gitlab.com/tangibleai/nlpia/-/raw/master/\"\n",
    "       \"src/nlpia/data/stopword_lists.json\")\n",
    "response = requests.get(url)\n",
    "stopwords = response.json()['exhaustive']  # <1>\n",
    "tokens = 'the words were just as I remembered them'.split()  # <2>\n",
    "tokens_without_stopwords = [x for x in tokens if x not in stopwords]\n",
    "print(tokens_without_stopwords)\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "len(stop_words)\n",
    "stop_words[:7]\n",
    "[sw for sw in stopwords if len(sw) == 1]\n",
    "resp = requests.get(url)\n",
    "len(resp.json()['exhaustive'])\n",
    "len(resp.json()['sklearn'])\n",
    "len(resp.json()['spacy'])\n",
    "len(resp.json()['nltk'])\n",
    "len(resp.json()['reuters'])\n",
    "tokens = ['House', 'Visitor', 'Center']\n",
    "normalized_tokens = [x.lower() for x in tokens]\n",
    "print(normalized_tokens)\n",
    "def stem(phrase):\n",
    "    return ' '.join([re.findall('^(.*ss|.*?)(s)?$',\n",
    "        word)[0][0].strip(\"'\") for word in phrase.lower().split()])\n",
    "stem('houses')\n",
    "stem(\"Doctor House's calls\")\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "' '.join([stemmer.stem(w).strip(\"'\") for w in\n",
    "  \"dish washer's fairly washed dishes\".split()])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "' '.join([stemmer.stem(w).strip(\"'\") for w in\n",
    "  \"dish washer's fairly washed dishes\".split()])\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize(\"better\")  # <1>\n",
    "lemmatizer.lemmatize(\"better\", pos=\"a\")  # <2>\n",
    "lemmatizer.lemmatize(\"good\", pos=\"a\")\n",
    "lemmatizer.lemmatize(\"goods\", pos=\"a\")\n",
    "lemmatizer.lemmatize(\"goods\", pos=\"n\")\n",
    "lemmatizer.lemmatize(\"goodness\", pos=\"n\")\n",
    "lemmatizer.lemmatize(\"best\", pos=\"a\")\n",
    "stemmer.stem('goodness')\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"better good goods goodness best\")\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_)\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sa = SentimentIntensityAnalyzer()\n",
    "sa.lexicon  # <1>\n",
    "[(tok, score) for tok, score in sa.lexicon.items()\n",
    "  if \" \" in tok]  # <4>\n",
    "sa.polarity_scores(text=\\\n",
    "  \"Python is very readable and it's great for NLP.\")\n",
    "sa.polarity_scores(text=\\\n",
    "  \"Python is not a bad choice for most applications.\")\n",
    "corpus = [\"Absolutely perfect! Love it! :-) :-) :-)\",\n",
    "          \"Horrible! Completely useless. :(\",\n",
    "          \"It was OK. Some good and some bad things.\"]\n",
    "for doc in corpus:\n",
    "    scores = sa.polarity_scores(doc)\n",
    "    print('{:+}: {}'.format(scores['compound'], doc))\n",
    "movies = pd.read_csv('https://proai.org/movie-reviews.csv.gz', \\\n",
    "    index_col=0)\n",
    "movies.head().round(2)\n",
    "movies.describe().round(2)\n",
    "import pandas as pd\n",
    "pd.options.display.width = 75  # <1>\n",
    "from nltk.tokenize import casual_tokenize  # <2>\n",
    "bags_of_words = []\n",
    "from collections import Counter  # <3>\n",
    "for text in movies.text:\n",
    "    bags_of_words.append(Counter(casual_tokenize(text)))\n",
    "df_bows = pd.DataFrame.from_records(bags_of_words)  # <4>\n",
    "df_bows = df_bows.fillna(0).astype(int)  # <5>\n",
    "df_bows.shape  # <6>\n",
    "df_bows.head()\n",
    "df_bows.head()[list(bags_of_words[0].keys())]\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "nb = nb.fit(df_bows, movies.sentiment > 0)  # <1>\n",
    "movies['pred_senti'] = (\n",
    "  nb.predict_proba(df_bows))[:, 1] * 8 - 4  # <2>\n",
    "movies['error'] = movies.pred_senti - movies.sentiment\n",
    "mae = movies['error'].abs().mean().round(1)  # <3>\n",
    "mae\n",
    "movies['senti_ispos'] = (movies['sentiment'] > 0).astype(int)\n",
    "movies['pred_ispos'] = (movies['pred_senti'] > 0).astype(int)\n",
    "columns = [c for c in movies.columns if 'senti' in c or 'pred' in c]\n",
    "movies[columns].head(8)\n",
    "(movies.pred_ispos ==\n",
    "  movies.senti_ispos).sum() / len(movies)\n",
    "products = pd.read_csv('https://proai.org/product-reviews.csv.gz')\n",
    "for text in products['text']:\n",
    "    bags_of_words.append(Counter(casual_tokenize(text)))\n",
    "df_product_bows = pd.DataFrame.from_records(bags_of_words)\n",
    "df_product_bows = df_product_bows.fillna(0).astype(int)\n",
    "df_all_bows = df_bows.append(df_product_bows)\n",
    "df_all_bows.columns  # <1>\n",
    "df_product_bows = df_all_bows.iloc[len(movies):][df_bows.columns]  # <2>\n",
    "df_product_bows.shape\n",
    "df_bows.shape  # <3>\n",
    "products['senti_ispos'] = (products['sentiment'] > 0).astype(int)\n",
    "products['pred_ispos'] = nb.predict(df_product_bows).astype(int)\n",
    "products.head()\n",
    "tp = products['pred_ispos'] == products['senti_ispos']  # <1>\n",
    "tp.sum() / len(products)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
