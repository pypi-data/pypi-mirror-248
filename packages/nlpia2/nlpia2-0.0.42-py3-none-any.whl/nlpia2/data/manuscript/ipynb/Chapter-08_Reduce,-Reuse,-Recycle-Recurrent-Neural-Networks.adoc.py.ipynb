{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974efd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self,\n",
    "            vocab_size, hidden_size, output_size):  # <1>\n",
    "        super().__init__()\n",
    "        self.W_c2h = nn.Linear(\n",
    "            vocab_size + hidden_size, hidden_size)  # <2>\n",
    "        self.W_c2y = nn.Linear(vocab_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    def forward(self, x, hidden):  # <3>\n",
    "        combined = torch.cat((x, hidden), axis=1)  # <4>\n",
    "        hidden = self.W_c2h(combined)  # <5>\n",
    "        y = self.W_c2y(combined)  # <6>\n",
    "        y = self.softmax(y)\n",
    "        return y, hidden  # <7>\n",
    "import pandas as pd\n",
    "from nlpia2.spacy_language_model import nlp\n",
    "tagged_tokens = list(nlp('Hello world. Goodbye now!'))\n",
    "interesting_tags = 'text dep_ head lang_ lemma_ pos_ sentiment'\n",
    "interesting_tags = (interesting_tags +  'shape_ tag_').split()\n",
    "pd.DataFrame([\n",
    "        [getattr(t, a) for a in interesting_tags]\n",
    "        for t in tagged_tokens],\n",
    "    columns=interesting_tags)\n",
    "from nlpia2.string_normalizers import Asciifier\n",
    "asciify = Asciifier()\n",
    "asciify(\"O’Néàl\")\n",
    "asciify(\"Çetin\")\n",
    "repo = 'tangibleai/nlpia2'  # <1>\n",
    "filepath = 'src/nlpia2/data/surname-nationality.csv.gz'\n",
    "url = f\"https://gitlab.com/{repo}/-/raw/main/{filepath}\"\n",
    "df = pd.read_csv(url)  # <2>\n",
    "df[['surname', 'nationality']].sort_values('surname').head(9)\n",
    "df['nationality'].nunique()\n",
    "sorted(df['nationality'].unique())\n",
    "fraction_unique = {}\n",
    "for i, g in df.groupby('nationality'):\n",
    "    fraction_unique[i] = g['surname'].nunique() / len(g)\n",
    "pd.Series(fraction_unique).sort_values().head(7)\n",
    "arabic = [x.strip() for x in open('.nlpia2-data/names/Arabic.txt')]\n",
    "arabic = pd.Series(sorted(arabic))\n",
    "df.groupby('surname')\n",
    "overlap = {}\n",
    "for i, g in df.groupby('surname'):\n",
    "    n = g['nationality'].nunique()\n",
    "    if n > 1:\n",
    "        overlap[i] = {'nunique': n, 'unique': list(g['nationality'].unique())}\n",
    "overlap.sort_values('nunique', ascending=False)\n",
    "class RNN(nn.Module):\n",
    "def __init__(self, n_hidden=128, categories, char2i):  # <1>\n",
    "    super().__init__()\n",
    "    self.categories = categories\n",
    "    self.n_categories = len(self.categories)  # <2>\n",
    "    print(f'RNN.categories: {self.categories}')\n",
    "    print(f'RNN.n_categories: {self.n_categories}')\n",
    "def forward(self, x, hidden):  # <3>\n",
    "    combined = torch.cat((x, hidden), 1)\n",
    "    hidden = self.W_c2h(combined)\n",
    "    y = self.W_c2y(combined)\n",
    "    y = self.softmax(y)\n",
    "    return y, hidden  # <4>\n",
    "def train_sample(model, category_tensor, char_seq_tens,\n",
    "                criterion=nn.NLLLoss(), lr=.005):\n",
    "%run classify_name_nationality.py  # <1>\n",
    "model.predict_category(\"Khalid\")\n",
    "predictions = topk_predictions(model, 'Khalid', topk=4)\n",
    "predictions\n",
    "predictions = topk_predictions(model, 'Khalid', topk=4)\n",
    "predictions['likelihood'] = np.exp(predictions['log_loss'])\n",
    "predictions\n",
    "def predict_hidden(self, text=\"Khalid\"):\n",
    "   text_tensor = self.encode_one_hot_seq(text)\n",
    "   with torch.no_grad():  # <1>\n",
    "   hidden = self.hidden_init\n",
    "       for i in range(text_tensor.shape[0]):  # <2>\n",
    "           y, hidden = self(text_tensor[i], hidden)  # <3>\n",
    "   return hidden\n",
    "def predict_proba(self, text=\"Khalid\"):\n",
    "   text_tensor = self.encode_one_hot_seq(text)\n",
    "   with torch.no_grad():\n",
    "       hidden = self.hidden_init\n",
    "       for i in range(text_tensor.shape[0]):\n",
    "           y, hidden = self(text_tensor[i], hidden)\n",
    "   return y  # <1>\n",
    "def predict_category(self, text):\n",
    "   tensor = self.encode_one_hot_seq(text)\n",
    "   y = self.predict_proba(tensor)  # <1>\n",
    "   pred_i = y.topk(1)[1][0].item()  # <2>\n",
    "   return self.categories[pred_i]\n",
    "text = 'Khalid'\n",
    "pred_categories = []\n",
    "pred_hiddens = []\n",
    "for i in range(1, len(text) + 1):\n",
    "   pred_hiddens.append(model.predict_hidden(text[:i]))  # <1>\n",
    "   pred_categories.append(model.predict_category(text[:i]))\n",
    "pd.Series(pred_categories, input_texts)\n",
    "hiddens = [h[0].tolist() for h in hiddens]\n",
    "df_hidden = pd.DataFrame(hidden_lists, index=list(text))\n",
    "df_hidden = df_hidden.T.round(2)  # <1>\n",
    "df_hidden\n",
    "position = pd.Series(range(len(text)), index=df_hidden.index)\n",
    "pd.DataFrame(position).T\n",
    "df_hidden_raw.corrwith(position).sort_values()\n",
    "lines = open('data/wikitext-2/train.txt').readlines()\n",
    "for line in lines[:4]:\n",
    "    print(line.rstrip()[:70])\n",
    "from nlpia2.ch08.data import Corpus\n",
    "corpus = Corpus('data/wikitext-2')\n",
    "corpus.train\n",
    "vocab = corpus.dictionary\n",
    "[vocab.idx2word[i] for i in corpus.train[:7]]\n",
    "def batchify_slow(x, batch_size=8, num_batches=5):\n",
    "   batches = []\n",
    "   for i in range(int(len(x)/batch_size)):\n",
    "       if i > num_batches:\n",
    "           break\n",
    "       batches.append(x[i*batch_size:i*batch_size + batch_size])\n",
    "   return batches\n",
    "batches = batchify_slow(corpus.train)\n",
    "batches\n",
    "torch.stack(batches)\n",
    "r = sigmoid(W_i2r.mm(x) + b_i2r +    W_h2r.mm(h) + b_h2r)  # <1>\n",
    "z = sigmoid(W_i2z.mm(x) + b_i2z +    W_h2z.mm(h) + b_h2z)  # <2>\n",
    "n =    tanh(W_i2n.mm(x) + b_i2n + r∗(W_h2n.mm(h) + b_h2n))  # <3>\n",
    "def count_parameters(model, learned=True):\n",
    "    return sum(\n",
    "        p.numel() for p in model.parameters()  # <1>\n",
    "        if not learned or p.requires_grad  # <2>\n",
    "    )\n",
    "import jsonlines  # <1>\n",
    "with jsonlines.open('experiments.jsonl') as fin:\n",
    "    lines = list(fin)\n",
    "df = pd.DataFrame(lines)\n",
    "df.to_csv('experiments.csv')\n",
    "cols = 'learned_parameters rnn_type epochs lr num_layers'\n",
    "cols += ' dropout epoch_time test_loss'\n",
    "cols = cols.split()\n",
    "df[cols].round(2).sort_values('test_loss', ascending=False)\n",
    "df\n",
    "from nlpia2.ch08.rnn_word.data import Corpus\n",
    "corpus = Corpus('data/wikitext-2')\n",
    "passage = corpus.train.numpy()[-89:-35]\n",
    "' '.join([vocab.idx2word[i] for i in passage])\n",
    "num_eos = sum([vocab.idx2word[i] == '<eos>' for i in\n",
    "num_eos\n",
    "num_unk = sum([vocab.idx2word[i] == '<unk>' for i in\n",
    "num_unk\n",
    "num_normal = sum([\n",
    "    vocab.idx2word[i] not in ('<unk>', '<eos>')\n",
    "    for i in corpus.train.numpy()])\n",
    "num_normal\n",
    "num_unk / (num_normal + num_eos + num_unk)\n",
    "import torch\n",
    "from preprocessing import Corpus\n",
    "from generate import generate_words\n",
    "from model import RNNModel\n",
    "corpus = Corpus('data/wikitext-2')\n",
    "vocab = corpus.dictionary\n",
    "with open('model.pt', 'rb') as f:\n",
    "   orig_model = torch.load(f, map_location='cpu')  # <1>\n",
    "model = RNNModel('GRU', vocab=corpus.dictionary, num_layers=1)  # <2>\n",
    "model.load_state_dict(orig_model.state_dict())\n",
    "words = generate_words(\n",
    "   model=model, vocab=vocab, prompt='The', temperature=.1)  # <3>\n",
    "print(' '.join(w for w in words))\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
