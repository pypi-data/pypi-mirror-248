{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ee4e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = 'https://gitlab.com/tangibleai/nlpia2/-/raw/main/src/nlpia2'\n",
    "url += '/data/llm/llm-emmergence-table-other-big-bench-tasks.csv'\n",
    "df = pd.read_csv(url, index_col=0)\n",
    "df.shape  # <1>\n",
    "df['Emergence'].value_counts()\n",
    "scales = df['Emergence'].apply(lambda x: 'line' in x or 'flat' in x)\n",
    "df[scales].sort_values('Task')  # <3>\n",
    "import numpy as np\n",
    "np.random.choice(\n",
    "    'statistical,AI,stochastic,interesting,a,an,in,of'.split(','),\n",
    "    p=[.18, .17, .15, .1, .1, .1, .1, .1])  # <1>\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "SEED = 42\n",
    "DEVICE = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.cuda.device(0)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED) # <1>\n",
    "from transformers import set_seed\n",
    "set_seed(SEED)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token  # <1>\n",
    "vanilla_gpt2 = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "def generate(prompt,\n",
    "       model=vanilla_gpt2,\n",
    "       tokenizer=tokenizer,\n",
    "       device=DEVICE, **kwargs):\n",
    "   encoded_prompt = tokenizer.encode(\n",
    "       prompt, return_tensors='pt')\n",
    "   encoded_prompt = encoded_prompt.to(device)\n",
    "   encoded_output = model.generate (encoded_prompt, **kwargs)\n",
    "   encoded_output = encoded_output.squeeze() # <1>\n",
    "   decoded_output = tokenizer.decode(encoded_output,\n",
    "       clean_up_tokenization_spaces=True,\n",
    "       skip_special_tokens=True)\n",
    "   return decoded_output\n",
    "generate(\n",
    "    model=vanilla_gpt2,\n",
    "    tokenizer=tokenizer,\n",
    "    prompt='NLP is',\n",
    "    max_length=50)\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "input_ids = input_ids.to(DEVICE)\n",
    "vanilla_gpt2(input_ids=input_ids)\n",
    "output = vanilla_gpt2(input_ids=input_ids)\n",
    "output.logits.shape\n",
    "encoded_prompt = tokenizer('NLP is a', return_tensors=\"pt\")\n",
    "encoded_prompt = encoded_prompt[\"input_ids\"]\n",
    "encoded_prompt = encoded_prompt.to(DEVICE)\n",
    "output = vanilla_gpt2(input_ids=encoded_prompt)\n",
    "next_token_logits = output.logits[0, -1, :]\n",
    "next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
    "sorted_ids = torch.argsort(next_token_probs, dim=-1, descending=True)\n",
    "tokenizer.decode(sorted_ids[0])  # <1>\n",
    "tokenizer.decode(sorted_ids[1])  # <2>\n",
    "kwargs = {\n",
    "   'do_sample': True,\n",
    "   'max_length': 50,\n",
    "   'top_p': 0.92\n",
    "}\n",
    "print(generate(prompt='NLP is a', **kwargs))\n",
    "import pandas as pd\n",
    "DATASET_URL = ('https://gitlab.com/tangibleai/nlpia2/'\n",
    "    '-/raw/main/src/nlpia2/data/nlpia_lines.csv')\n",
    "df = pd.read_csv(DATASET_URL)\n",
    "df = df[df['is_text']]\n",
    "lines = df.line_text.copy()\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import random_split\n",
    "class NLPiADataset(Dataset):\n",
    "    def __init__(self, txt_list, tokenizer, max_length=768):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        self.attn_masks = []\n",
    "        for txt in txt_list:\n",
    "            encodings_dict = tokenizer(txt, truncation=True,\n",
    "                max_length=max_length, padding=\"max_length\")\n",
    "            self.input_ids.append(\n",
    "                torch.tensor(encodings_dict['input_ids']))\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx]\n",
    "dataset = NLPiADataset(lines, tokenizer, max_length=768)\n",
    "train_size = int(0.9 * len(dataset))\n",
    "eval_size = len(dataset) - train_size\n",
    "train_dataset, eval_dataset = random_split(\n",
    "    dataset, [train_size, eval_size])\n",
    "from nlpia2.constants import DATA_DIR  # <1>\n",
    "from transformers import TrainingArguments\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "training_args = TrainingArguments(\n",
    "   output_dir=DATA_DIR / 'ch10_checkpoints',\n",
    "   per_device_train_batch_size=5,\n",
    "   num_train_epochs=5,\n",
    "   save_strategy='epoch')\n",
    "collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False)  # <2>\n",
    "from transformers import Trainer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")  # <1>\n",
    "trainer = Trainer(\n",
    "       model,\n",
    "       training_args,\n",
    "       data_collator=collator,       # <2>\n",
    "       train_dataset=train_dataset,  # <3>\n",
    "       eval_dataset=eval_dataset)\n",
    "trainer.train()\n",
    "generate('NLP is')\n",
    "print(generate(\"Neural networks\", **nucleus_sampling_args))\n",
    "print(generate(\"Neural networks\", **nucleus_sampling_args))\n",
    "import numpy as np\n",
    "v = np.array([1.1, 2.22, 3.333, 4.4444, 5.55555])\n",
    "type(v[0])\n",
    "(v * 1_000_000).astype(np.int32)\n",
    "v = (v * 1_000_000).astype(np.int32)  # <1>\n",
    "v = (v + v) // 2\n",
    "v / 1_000_000\n",
    "v = np.array([1.1, 2.22, 3.333, 4.4444, 5.55555])\n",
    "v = (v * 10_000).astype(np.int16)  # <1>\n",
    "v = (v + v) // 2\n",
    "v / 10_000\n",
    "v = np.array([1.1, 2.22, 3.333, 4.4444, 5.55555])\n",
    "v = (v * 1_000).astype(np.int16)  # <3>\n",
    "v = (v + v) // 2\n",
    "v / 1_000\n",
    "import pandas as pd\n",
    "DATASET_URL = ('https://gitlab.com/tangibleai/nlpia2/'\n",
    "    '-/raw/main/src/nlpia2/data/nlpia_lines.csv')\n",
    "df = pd.read_csv(DATASET_URL)\n",
    "df = df[df['is_text']]\n",
    "from haystack import Document\n",
    "titles = list(df[\"line_text\"].values)\n",
    "texts = list(df[\"line_text\"].values)\n",
    "documents = []\n",
    "for title, text in zip(titles, texts):\n",
    "   documents.append(Document(content=text, meta={\"name\": title or \"\"}))\n",
    "documents[0]\n",
    "from haystack.document_stores import FAISSDocumentStore\n",
    "document_store = FAISSDocumentStore(\n",
    "    return_embedding=True)  # <1>\n",
    "document_store.write_documents(documents)\n",
    "from haystack.nodes import TransformersReader, EmbeddingRetriever\n",
    "reader = TransformersReader(model_name_or_path\n",
    "    =\"deepset/roberta-base-squad2\")  # <1>\n",
    "retriever = EmbeddingRetriever(\n",
    "   document_store=document_store,\n",
    "   embedding_model=\"sentence-transformers/multi-qa-mpnet-base-dot-v1\")\n",
    "document_store.update_embeddings(retriever=retriever)\n",
    "document_store.save('nlpia_index_faiss')  # <2>\n",
    "from haystack.pipelines import Pipeline\n",
    "pipe = Pipeline()\n",
    "pipe.add_node(component=retriever, name=\"Retriever\", inputs=[\"Query\"])\n",
    "pipe.add_node(component=reader, name=\"Reader\", inputs=[\"Retriever\"])\n",
    "from haystack.pipelines import ExtractiveQAPipeline\n",
    "pipe= ExtractiveQAPipeline(reader, retriever)\n",
    "question = \"What is an embedding?\"\n",
    "result = pipe.run(query=question,\n",
    "    params={\"Generator\": {\n",
    "        \"top_k\": 1}, \"Retriever\": {\"top_k\": 5}})\n",
    "print_answers(result, details='minimum')\n",
    "from haystack.nodes import Seq2SeqGenerator\n",
    "from haystack.pipelines import GenerativeQAPipeline\n",
    "generator = Seq2SeqGenerator(\n",
    "    model_name_or_path=\"vblagoje/bart_lfqa\",\n",
    "    max_length=200)\n",
    "pipe = GenerativeQAPipeline(generator, retriever)\n",
    "question = \"How CNNs are different from RNNs\"\n",
    "result = pipe.run( query=question,\n",
    "       params={\"Retriever\": {\"top_k\": 10}})  # <1>\n",
    "print_answers(result, details='medium')\n",
    "question = \"How can artificial intelligence save the world\"\n",
    "result = pipe.run(\n",
    "    query=\"How can artificial intelligence save the world\",\n",
    "    params={\"Retriever\": {\"top_k\": 10}})\n",
    "result\n",
    "import streamlit as st\n",
    "st.title(\"Ask me about NLPiA!\")\n",
    "st.markdown(\"Welcome to the official Question Answering webapp\"\n",
    "    \"for _Natural Language Processing in Action, 2nd Ed_\")\n",
    "question = st.text_input(\"Enter your question here:\")\n",
    "if question:\n",
    "   st.write(f\"You asked: '{question}'\")\n",
    "def load_store():\n",
    "  return FAISSDocumentStore.load(index_path=\"nlpia_faiss_index.faiss\",\n",
    "                                 config_path=\"nlpia_faiss_index.json\")\n",
    "@st.cache_resource\n",
    "def load_retriever(_document_store):    #<1>\n",
    "   return EmbeddingRetriever(\n",
    "    document_store=_document_store,\n",
    "    embedding_model=\"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
    "   )\n",
    "@st.cache_resource\n",
    "def load_reader():\n",
    "   return TransformersReader(\n",
    "       model_name_or_path=\"deepset/roberta-base-squad2\")\n",
    "document_store = load_store()\n",
    "extractive_retriever = load_retriever(document_store)\n",
    "reader = load_reader()\n",
    "pipe = ExtractiveQAPipeline(reader, extractive_retriever)\n",
    "if question:\n",
    "   res = pipe.run(query=question, params={\n",
    "import nlpia2_wikipedia.wikipedia as wiki\n",
    "wiki.page(\"AI\")\n",
    "import nlpia2_wikipedia.wikipedia as wiki\n",
    "page = wiki.page('AI')\n",
    "page.title\n",
    "print(page.content)\n",
    "wiki.search('AI')\n",
    "wiki.set_lang('zh')\n",
    "wiki.search('AI')\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
