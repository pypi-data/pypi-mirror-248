{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb349105",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.split(r'[!.?]+[ $]', \"Hello World.... Are you there?!?! I'm going to Mars!\")\n",
    "re.split(r'[!.?] ', \"The author wrote \\\"'I don't think it's conscious.' Turing said.\\\"\")\n",
    "re.split(r'[!.?] ', \"The author wrote \\\"'I don't think it's conscious.' Turing said.\\\" But I stopped reading.\")\n",
    "re.split(r'(?<!\\d)\\.|\\.(?!\\d)', \"I went to GT.You?\")\n",
    "import spacy\n",
    "nlpsm = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlpsm(\"This is a sentence. This is another sentence.\")\n",
    "for sent in doc.sents:\n",
    "  print(sent.text)\n",
    "import spacy\n",
    "nlp_statistical = spacy.load(\"en_core_web_sm\", exclude=[\"parser\"])\n",
    "nlp_statistical.enable_pipe(\"senter\")\n",
    "doc = nlp_statistical(\"This is a sentence. This is another sentence.\")\n",
    "for sent in doc.sents:\n",
    "   print(sent.text)\n",
    "from nlpia2 import wikipedia as wiki\n",
    "page = wiki.page('Timnit Gebru')\n",
    "text = page.content\n",
    "text[:66]\n",
    "i1 = text.index('Stochastic')\n",
    "text[i1:i1+51]\n",
    "doc = nlpsm(text)\n",
    "doc.ents[:6]  # <1>\n",
    "first_sentence = list(doc.sents)[0]\n",
    "' '.join(['{}_{}'.format(tok, tok.pos_) for tok in first_sentence])\n",
    "spacy.explain('CCONJ')\n",
    "' '.join(['{}_{}'.format(tok, tok.tag_) for tok in first_sentence])\n",
    "spacy.explain('VBZ')\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "def token_dict(token):\n",
    "   return OrderedDict( TOK=token.text,\n",
    "       POS=token.pos_, TAG=token.tag_,\n",
    "       ENT_TYPE=token.ent_type_, DEP=token.dep_,)\n",
    "def doc_df(doc):\n",
    "   return pd.DataFrame([token_dict(tok) for tok in doc])\n",
    "doc_df(doc)\n",
    "from nlpia2.spacy_pipes import nlp_df\n",
    "nlp = load('en_core_web_lg')\n",
    "tags = []\n",
    "for tok in doc:\n",
    "    tags.append(dict(token=tok.text, pos=tok.pos_, dep=tok.dep_))\n",
    "    tags[-1].update({f'child{i}': c.text for (i, c) in enumerate(tok.children)})\n",
    "df = pd.DataFrame(tags).set_index('token').fillna('')\n",
    "df.head()\n",
    "doc.ents\n",
    "doc\n",
    "from nlpia2.spacy_language_model import nlp\n",
    "import pandas as pd\n",
    "text = \"Gebru was unethically fired from her Ethical AI team.\"\n",
    "doc = nlp(text)\n",
    "tags = []\n",
    "for tok in doc:\n",
    "    tags.append(dict(text=tok.text, pos=tok.pos_, dep=tok.dep_))\n",
    "    tags[-1].update({f'child_{i}': c.text for (i, c) in enumerate(tok.children)})\n",
    "df =\n",
    "df = pd.DataFrame(tags)\n",
    "df\n",
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 20\n",
    "from nlpia2.nell import read_nell_tsv, simplify_names\n",
    "df = read_nell_tsv(nrows=1000)\n",
    "df[df.columns[:4]].head()\n",
    "pd.options.display.max_colwidth = 40\n",
    "df['entity'].str.split(':').str[1:].str.join(':')\n",
    "df['entity'].str.split(':').str[-1]\n",
    "df = simplify_names(df)  # <1>\n",
    "df[df.columns[[0, 1, 2, 4]]].head()\n",
    "islatlon = df['relation'] == 'latlon'\n",
    "df[islatlon].head()\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "sentence = \"We will be learning NLP today!\"\n",
    "print (\"{:<15} | {:<8} | {:<15} | {:<30} | {:<20}\".format('Token','Relation','Head', 'Children', 'Meaning'))\n",
    "print (\"-\" * 115)\n",
    "for token in doc:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d300ec4",
   "metadata": {},
   "source": [
    "Print the token, dependency nature, head, all dependents of the token, and meaning of the dependency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10d0a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "    print (\"{:<15} | {:<8} | {:<15} | {:<30} | {:<20}\"\n",
    "            .format(str(token.text), str(token.dep_), str(token.head.text), str([child for child in token.children]) , str(spacy.explain(token.dep_))[:17] ))\n",
    "import benepar\n",
    "benepar.download('benepar_en3')\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "if spacy.__version__.startswith('2'):\n",
    "    nlp.add_pipe(benepar.BeneparComponent(\"benepar_en3\"))\n",
    "else:\n",
    "    nlp.add_pipe(\"benepar\", config={\"model\": \"benepar_en3\"})\n",
    "doc = nlp(\"Johnson was compelled to ask the EU for an extension of the deadline, which was granted\")\n",
    "sent = list(doc.sents)[0]\n",
    "print(sent._.parse_string)\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "sent = \"John Smith works at Tangible AI\"\n",
    "doc = nlp(sent)\n",
    "entities = []\n",
    "for ent in doc.ents:\n",
    "    sent = sent.replace(ent.text, \"^/\" + ent.label_ + \"/\" + ent.text + \"^\")\n",
    "print(sent)\n",
    "i0 = text.index('Gebru had')\n",
    "text[i0:i0+171]\n",
    "import spacy, coreferee\n",
    "nlptrf = spacy.load('en_coreference_web_trf')\n",
    "text_gebru = text[i0:i1]\n",
    "doc_gebru = nlp(text_gebru)\n",
    "doc_gebru\n",
    "doc_gebru.spans\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "import neuralcoref\n",
    "neuralcoref.add_to_pipe(nlp)\n",
    "doc = nlp(u'My sister has a dog. She loves him.')\n",
    "doc._.coref_clusters\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "import allennlp_models.tagging\n",
    "predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2021.03.10.tar.gz\")\n",
    "predictor.predict(\n",
    "def find_greeting(s):\n",
    "    \"\"\" Return greeting str (Hi, etc) if greeting pattern matches \"\"\"\n",
    "    if s[0] == 'H':\n",
    "        if s[:3] in ['Hi', 'Hi ', 'Hi,', 'Hi!']:\n",
    "            return s[:2]\n",
    "        elif s[:6] in ['Hello', 'Hello ', 'Hello,', 'Hello!']:\n",
    "            return s[:5]\n",
    "    elif s[0] == 'Y':\n",
    "        if s[1] == 'o' and s[:3] in ['Yo', 'Yo,', 'Yo ', 'Yo!']:\n",
    "            return s[:2]\n",
    "    return None\n",
    "find_greeting('Hi Mr. Turing!')\n",
    "find_greeting('Hello, Rosa.')\n",
    "find_greeting(\"Yo, what's up?\")\n",
    "find_greeting(\"Hello\")\n",
    "print(find_greeting(\"hello\"))\n",
    "print(find_greeting(\"HelloWorld\"))\n",
    "import re\n",
    "lat = r'([-]?[0-9]?[0-9][.][0-9]{2,10})'\n",
    "lon = r'([-]?1?[0-9]?[0-9][.][0-9]{2,10})'\n",
    "sep = r'[,/ ]{1,3}'\n",
    "re_gps = re.compile(lat + sep + lon)\n",
    "re_gps.findall('http://...maps/@34.0551066,-118.2496763...')\n",
    "re_gps.findall(\"https://www.openstreetmap.org/#map=10/5.9666/116.0566\")\n",
    "re_gps.findall(\"Zig Zag Cafe is at 45.344, -121.9431 on my GPS.\")\n",
    "us = r'((([01]?\\d)[-/]([0123]?\\d))([-/]([0123]\\d)\\d\\d)?)'\n",
    "mdy = re.findall(us, 'Santa came 12/25/2017. An elf appeared 12/12.')\n",
    "mdy\n",
    "dates = [{'mdy': x[0], 'my': x[1], 'm': int(x[2]), 'd': int(x[3]),\n",
    "    'y': int(x[4].lstrip('/') or 0), 'c': int(x[5] or 0)} for x in mdy]\n",
    "dates\n",
    "for i, d in enumerate(dates):\n",
    "    for k, v in d.items():\n",
    "        if not v:\n",
    "            d[k] = dates[max(i - 1, 0)][k]  # <1>\n",
    "dates\n",
    "from datetime import date\n",
    "datetimes = [date(d['y'], d['m'], d['d']) for d in dates]\n",
    "datetimes\n",
    "eu = r'((([0123]?\\d)[-/]([01]?\\d))([-/]([0123]\\d)?\\d\\d)?)'\n",
    "dmy = re.findall(eu, 'Alan Mathison Turing OBE FRS (23/6/1912-7/6/1954) \\\n",
    "    was an English computer scientist.')\n",
    "dmy\n",
    "dmy = re.findall(eu, 'Alan Mathison Turing OBE FRS (23/6/12-7/6/54) \\\n",
    "    was an English computer scientist.')\n",
    "dmy\n",
    "yr_19xx = (\n",
    "    r'\\b(?P<yr_19xx>' +\n",
    "    '|'.join('{}'.format(i) for i in range(30, 100)) +\n",
    "    r')\\b'\n",
    "    )  # <1>\n",
    "yr_20xx = (\n",
    "    r'\\b(?P<yr_20xx>' +\n",
    "    '|'.join('{:02d}'.format(i) for i in range(10)) + '|' +\n",
    "    '|'.join('{}'.format(i) for i in range(10, 30)) +\n",
    "    r')\\b'\n",
    "    )  # <2>\n",
    "yr_cent = r'\\b(?P<yr_cent>' + '|'.join(\n",
    "    '{}'.format(i) for i in range(1, 40)) + r')'  # <3>\n",
    "yr_ccxx = r'(?P<yr_ccxx>' + '|'.join(\n",
    "    '{:02d}'.format(i) for i in range(0, 100)) + r')\\b'  # <4>\n",
    "yr_xxxx = r'\\b(?P<yr_xxxx>(' + yr_cent + ')(' + yr_ccxx + r'))\\b'\n",
    "yr = (\n",
    "    r'\\b(?P<yr>' +\n",
    "    yr_19xx + '|' + yr_20xx + '|' + yr_xxxx +\n",
    "    r')\\b'\n",
    "    )\n",
    "groups = list(re.finditer(\n",
    "    yr, \"0, 2000, 01, '08, 99, 1984, 2030/1970 85 47 `66\"))\n",
    "full_years = [g['yr'] for g in groups]\n",
    "full_years\n",
    "mon_words = 'January February March April May June July ' \\\n",
    "    'August September October November December'\n",
    "mon = (r'\\b(' + '|'.join('{}|{}|{}|{}|{:02d}'.format(\n",
    "    m, m[:4], m[:3], i + 1, i + 1) for i, m in enumerate(mon_words.split())) +\n",
    "    r')\\b')\n",
    "re.findall(mon, 'January has 31 days, February the 2nd month of 12, has 28, except in a Leap Year.')\n",
    "day = r'|'.join('{:02d}|{}'.format(i, i) for i in range(1, 32))\n",
    "eu = (r'\\b(' + day + r')\\b[-,/ ]{0,2}\\b(' +\n",
    "    mon + r')\\b[-,/ ]{0,2}\\b(' + yr.replace('<yr', '<eu_yr') + r')\\b')\n",
    "us = (r'\\b(' + mon + r')\\b[-,/ ]{0,2}\\b(' +\n",
    "    day + r')\\b[-,/ ]{0,2}\\b(' + yr.replace('<yr', '<us_yr') + r')\\b')\n",
    "date_pattern = r'\\b(' + eu + '|' + us + r')\\b'\n",
    "list(re.finditer(date_pattern, '31 Oct, 1970 25/12/2017'))\n",
    "import datetime\n",
    "dates = []\n",
    "for g in groups:\n",
    "    month_num = (g['us_mon'] or g['eu_mon']).strip()\n",
    "    try:\n",
    "        month_num = int(month_num)\n",
    "    except ValueError:\n",
    "        month_num = [w[:len(month_num)]\n",
    "            for w in mon_words].index(month_num) + 1\n",
    "    date = datetime.date(\n",
    "        int(g['us_yr'] or g['eu_yr']),\n",
    "        month_num,\n",
    "        int(g['us_day'] or g['eu_day']))\n",
    "    dates.append(date)\n",
    "dates\n",
    "from spacy.displacy import render\n",
    "sentence = \"In 1541 Desoto wrote in his journal about the Pascagoula.\"\n",
    "parsed_sent = nlp(sentence)\n",
    "with open('pascagoula.html', 'w') as f:\n",
    "    f.write(render(docs=parsed_sent, page=True, options=dict(compact=True)))\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "def token_dict(token):\n",
    "    return OrderedDict(ORTH=token.orth_, LEMMA=token.lemma_,\n",
    "        POS=token.pos_, TAG=token.tag_, DEP=token.dep_)\n",
    "def doc_dataframe(doc):\n",
    "    return pd.DataFrame([token_dict(tok) for tok in doc])\n",
    "doc_dataframe(nlp(\"In 1541 Desoto met the Pascagoula.\"))\n",
    "pattern = [\n",
    "    {'TAG': 'NN', 'OP': '+'},\n",
    "    {'IS_ALPHA': True, 'OP': '*'},\n",
    "    {'LEMMA': 'meet'},\n",
    "    {'IS_ALPHA': True, 'OP': '*'},\n",
    "    {'TAG': 'NN', 'OP': '+'}]\n",
    "from spacy.matcher import Matcher\n",
    "doc = nlp(\"In 1541 Desoto met the Pascagoula.\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\n",
    "    key='met',\n",
    "    patterns=[pattern])\n",
    "matches = matcher(doc)\n",
    "matches\n",
    "start = matches[0][1]\n",
    "stop = matches[0][2]\n",
    "doc[start:stop]  # <2>\n",
    "doc = nlp(\"October 24: Lewis and Clark met their\" \\\n",
    "    \"first Mandan Chief, Big White.\")\n",
    "m = matcher(doc)[0]\n",
    "m\n",
    "doc[m[1]:m[2]]\n",
    "doc = nlp(\"On 11 October 1986, Gorbachev and Reagan met at Höfði house\")\n",
    "matcher(doc)\n",
    "doc = nlp(\n",
    "    \"On 11 October 1986, Gorbachev and Reagan met at Hofoi house\"\n",
    "    )\n",
    "pattern = [\n",
    "    {'TAG': 'NN', 'OP': '+'},\n",
    "    {'LEMMA': 'and'},\n",
    "    {'TAG': 'NN', 'OP': '+'},\n",
    "    {'IS_ALPHA': True, 'OP': '*'},\n",
    "    {'LEMMA': 'meet'}\n",
    "    ]\n",
    "matcher.add('met', None, pattern)  # <1>\n",
    "matches = matcher(doc)\n",
    "pd.DataFrame(matches, columns=)\n",
    "doc[m[-1][1]:m[-1][2]]  # <3>\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
