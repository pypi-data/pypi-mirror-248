{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8562d157",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nessvec.indexers import Index  # <1>\n",
    "index = Index(num_vecs=100_000)  # <2>\n",
    "index.get_nearest(\"Engineer\").round(2)\n",
    "index.get_nearest(\"Programmer\").round(2)\n",
    "index.get_nearest(\"Developer\").round(2)\n",
    "\"Chief\" + \"Engineer\"\n",
    "\"Chief\" + \" \" + \"Engineer\"\n",
    "chief = (index.data[index.vocab[\"Chief\"]]\n",
    "    + index.data[index.vocab[\"Engineer\"]])\n",
    "index.get_nearest(chief)\n",
    "answer_vector = wv['woman'] + wv['Europe'] + wv['physics'] +\n",
    "    wv['scientist']\n",
    "answer_vector = wv['woman'] + wv['Europe'] + wv['physics'] +\\\n",
    "    wv['scientist'] - wv['male'] - 2 * wv['man']\n",
    "answer_vector = wv['Louis_Pasteur'] - wv['germs'] + wv['physics']\n",
    "wv['Marie_Curie'] - wv['science'] + wv['music']\n",
    "from nessvec.examples.ch06.nessvectors import *  # <1>\n",
    "nessvector('Marie_Curie').round(2)\n",
    "import torchtext\n",
    "dsets = torchtext.datasets.WikiText2()\n",
    "num_texts = 10000\n",
    "filepath = DATA_DIR / f'WikiText2-{num_texts}.txt'\n",
    "with open(filepath, 'wt') as fout:\n",
    "    fout.writelines(list(dsets[0])[:num_texts])\n",
    "!tail -n 3 ~/nessvec-data/WikiText2-10000.txt\n",
    "import datasets\n",
    "dset = datasets.load_dataset('text', data_files=str(filepath))\n",
    "dset\n",
    "dset = dset.map(tokenize_row)\n",
    "dset\n",
    "vocab = list(set(\n",
    "    [tok for row in dset['train']['tokens'] for tok in row]))\n",
    "vocab[:4]\n",
    "id2tok = dict(enumerate(vocab))\n",
    "list(id2tok.items())[:4]\n",
    "tok2id = {tok: i for (i, tok) in id2tok.items()}\n",
    "list(tok2id.items())[:4]\n",
    "def windowizer(row, wsize=WINDOW_WIDTH):\n",
    "dset = dset.map(windowizer)\n",
    "dset\n",
    "def skip_grams(tokens, window_width=WINDOW_WIDTH):\n",
    "   pairs = []\n",
    "   for i, wd in enumerate(tokens):\n",
    "       target = tok2id[wd]\n",
    "       window = [\n",
    "           i + j for j in\n",
    "           range(-window_width, window_width + 1, 1)\n",
    "           if (i + j >= 0)\n",
    "           & (i + j < len(tokens))\n",
    "           & (j != 0)\n",
    "       ]\n",
    "from torch.utils.data import Dataset\n",
    "class Word2VecDataset(Dataset):\n",
    "   def __init__(self, dataset, vocab_size, wsize=WINDOW_WIDTH):\n",
    "       self.dataset = dataset\n",
    "       self.vocab_size = vocab_size\n",
    "       self.data = [i for s in dataset['moving_window'] for i in s]\n",
    "   def __len__(self):\n",
    "       return len(self.data)\n",
    "   def __getitem__(self, idx):\n",
    "       return self.data[idx]\n",
    "model = Word2Vec()\n",
    "model\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "device\n",
    "model.to(device)\n",
    "from tqdm import tqdm  # noqa\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 5e-4\n",
    "from nlpia.data.loaders import get_data\n",
    "word_vectors = get_data('word2vec')\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "word_vectors = KeyedVectors.load_word2vec_format(\\\n",
    "    '/path/to/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from nlpia.loaders import get_data\n",
    "word_vectors = get_data('w2v', limit=200000)  # <1>\n",
    "word_vectors.most_similar(positive=['cooking', 'potatoes'], topn=5)\n",
    "word_vectors.most_similar(positive=['germany', 'france'], topn=1)\n",
    "word_vectors.doesnt_match(\"potatoes milk cake computer\".split())\n",
    "word_vectors.most_similar(positive=['king', 'woman'],\n",
    "    negative=['man'], topn=2)\n",
    "word_vectors.similarity('princess', 'queen')\n",
    "word_vectors['phone']\n",
    "token_list\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "num_features = 300  # <1>\n",
    "min_word_count = 3  # <2>\n",
    "num_workers = 2  # <3>\n",
    "window_size = 6  # <4>\n",
    "subsampling = 1e-3  # <5>\n",
    "model = Word2Vec(\n",
    "    token_list,\n",
    "    workers=num_workers,\n",
    "    size=num_features,\n",
    "    min_count=min_word_count,\n",
    "    window=window_size,\n",
    "    sample=subsampling)\n",
    "model.init_sims(replace=True)\n",
    "model_name = \"my_domain_specific_word2vec_model\"\n",
    "model.save(model_name)\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "model_name = \"my_domain_specific_word2vec_model\"\n",
    "model = Word2Vec.load(model_name)\n",
    "model.most_similar('radiology')\n",
    "from nessvec.files import load_fasttext\n",
    "df = load_fasttext()  # <1>\n",
    "df.head().round(2)\n",
    "df.loc['prosocial']  # <2>\n",
    "from nessvec.indexers import Index\n",
    "index = Index()  # <1>\n",
    "vecs = index.vecs\n",
    "vecs.shape\n",
    "import pandas as pd\n",
    "vocab = pd.Series(wv.vocab)\n",
    "vocab.iloc[1000000:100006]\n",
    "wv['Illini']\n",
    "import numpy as np\n",
    "np.linalg.norm(wv['Illinois'] - wv['Illini'])  # <1>\n",
    "cos_similarity = np.dot(wv['Illinois'], wv['Illini']) / (\n",
    "    np.linalg.norm(wv['Illinois']) *\\\n",
    "    np.linalg.norm(wv['Illini']))  # <2>\n",
    "cos_similarity\n",
    "1 - cos_similarity # <3>\n",
    "from nlpia.data.loaders import get_data\n",
    "cities = get_data('cities')\n",
    "cities.head(1).T\n",
    "us = cities[(cities.country_code == 'US') &\\\n",
    "    (cities.admin1_code.notnull())].copy()\n",
    "states = pd.read_csv(\\\n",
    "    'http://www.fonz.net/blog/wp-content/uploads/2008/04/states.csv')\n",
    "states = dict(zip(states.Abbreviation, states.State))\n",
    "us['city'] = us.name.copy()\n",
    "us['st'] = us.admin1_code.copy()\n",
    "us['state'] = us.st.map(states)\n",
    "us[us.columns[-3:]].head()\n",
    "vocab = pd.np.concatenate([us.city, us.st, us.state])\n",
    "vocab = np.array([word for word in vocab if word in wv.wv])\n",
    "vocab[:10]\n",
    "city_plus_state = []\n",
    "for c, state, st in zip(us.city, us.state, us.st):\n",
    "    if c not in vocab:\n",
    "        continue\n",
    "    row = []\n",
    "    if state in vocab:\n",
    "        row.extend(wv[c] + wv[state])\n",
    "    else:\n",
    "        row.extend(wv[c] + wv[st])\n",
    "    city_plus_state.append(row)\n",
    "us_300D = pd.DataFrame(city_plus_state)\n",
    "word_model.distance('man', 'nurse')\n",
    "word_model.distance('woman', 'nurse')\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)  # <1>\n",
    "us_300D = get_data('cities_us_wordvectors')\n",
    "us_2D = pca.fit_transform(us_300D.iloc[:, :300])  # <2>\n",
    "import seaborn\n",
    "from matplotlib import pyplot as plt\n",
    "from nlpia.plots import offline_plotly_scatter_bubble\n",
    "df = get_data('cities_us_wordvectors_pca2_meta')\n",
    "html = offline_plotly_scatter_bubble(\n",
    "    df.sort_values('population', ascending=False)[:350].copy()\\\n",
    "        .sort_values('population'),\n",
    "    filename='plotly_scatter_bubble.html',\n",
    "    x='x', y='y',\n",
    "    size_col='population', text_col='name', category_col='timezone',\n",
    "    xscale=None, yscale=None,  # 'log' or None\n",
    "    layout={}, marker={'sizeref': 3000})\n",
    "import requests\n",
    "repo = 'https://gitlab.com/tangibleai/nlpia2/-/raw/main'\n",
    "name = 'Chapter-06_Reasoning-with-word-embeddings-word-vectors.adoc'\n",
    "url = f'{repo}/src/nlpia2/data/{name}'\n",
    "adoc_text = requests.get(url)\n",
    "from pathlib import Path\n",
    "path = Path.cwd() / name\n",
    "with path.open('w') as fout:\n",
    "    fout.write(adoc_text)\n",
    "import subprocess\n",
    "subprocess.run(args=[   # <1>\n",
    "    'asciidoc3', '-a', '-n', '-a', 'icons', path.name])\n",
    "if os.path.exists(chapt6_html) and os.path.getsize(chapt6_html) > 0:\n",
    "    chapter6_html = open(chapt6_html, 'r').read()\n",
    "    bsoup = BeautifulSoup(chapter6_html, 'html.parser')\n",
    "    text = bsoup.get_text()  # <1>\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "config = {'punct_chars': None}\n",
    "nlp.add_pipe('sentencizer', config=config)\n",
    "doc = nlp(text)\n",
    "sentences = []\n",
    "noun_phrases = []\n",
    "for sent in doc.sents:\n",
    "    sent_noun_chunks = list(sent.noun_chunks)\n",
    "    if sent_noun_chunks:\n",
    "        sentences.append(sent)\n",
    "        noun_phrases.append(max(sent_noun_chunks))\n",
    "sent_vecs = []\n",
    "for sent in sentences:\n",
    "   sent_vecs.append(sent.vector)\n",
    "import numpy as np\n",
    "vector = np.array([1, 2, 3, 4])  # <1>\n",
    "np.sqrt(sum(vector**2))\n",
    "np.linalg.norm(vector)  # <2>\n",
    "import numpy as np\n",
    "for i, sent_vec in enumerate(sent_vecs):\n",
    "    sent_vecs[i] = sent_vec / np.linalg.norm(sent_vec)\n",
    "np_array_sent_vecs_norm = np.array(sent_vecs)\n",
    "similarity_matrix = np_array_sent_vecs_norm.dot(\n",
    "    np_array_sent_vecs_norm.T)  # <1>\n",
    "import re\n",
    "import networkx as nx\n",
    "similarity_matrix = np.triu(similarity_matrix, k=1)  # <1>\n",
    "iterator = np.nditer(similarity_matrix, flags=['multi_index'],\n",
    "node_labels = dict()\n",
    "G = nx.Graph()\n",
    "pattern = re.compile(\n",
    "   r'[\\w\\s]*[\\'\\\"]?[\\w\\s]+\\-?[\\w\\s]*[\\'\\\"]?[\\w\\s]*'\n",
    "   )  # <2>\n",
    "for edge in iterator:\n",
    "    key = 0\n",
    "    value = ''\n",
    "    if edge > 0.95:  # <3>\n",
    "        key = iterator.multi_index[0]\n",
    "        value = str(noun_phrases[iterator.multi_index[0]])\n",
    "        if pattern.fullmatch(value)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.subplot(1, 1, 1)  # <1>\n",
    "pos = nx.spring_layout(G, k=0.15, seed=42)  # <1>\n",
    "nx.draw_networkx(G, pos=pos, with_labels=True, labels=node_labels,\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
