{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f2aed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model=512, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)  # <1>\n",
    "        self.d_model = d_model  # <2>\n",
    "        self.max_len = max_len  # <3>\n",
    "        pe = torch.zeros(max_len, d_model)  # <4>\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n",
    "                             (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # <5>\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]  # <6>\n",
    "        return self.dropout(x)\n",
    "from datasets import load_dataset  # <1>\n",
    "opus = load_dataset('opus_books', 'de-en')\n",
    "opus\n",
    "sents = opus['train'].train_test_split(test_size=.1)\n",
    "sents\n",
    "next(iter(sents['test']))  # <1>\n",
    "DEVICE = torch.device(\n",
    "    'cuda' if torch.cuda.is_available()\n",
    "    else 'cpu')\n",
    "SRC = 'en'  # <1>\n",
    "TGT = 'de'  # <2>\n",
    "SOS, EOS = '<s>', '</s>'\n",
    "PAD, UNK, MASK = '<pad>', '<unk>', '<mask>'\n",
    "SPECIAL_TOKS = [SOS, PAD, EOS, UNK, MASK]\n",
    "VOCAB_SIZE = 10_000\n",
    "from tokenizers import ByteLevelBPETokenizer  # <3>\n",
    "tokenize_src = ByteLevelBPETokenizer()\n",
    "tokenize_src.train_from_iterator(\n",
    "    [x[SRC] for x in sents['train']['translation']],\n",
    "    vocab_size=10000, min_frequency=2,\n",
    "    special_tokens=SPECIAL_TOKS)\n",
    "PAD_IDX = tokenize_src.token_to_id(PAD)\n",
    "tokenize_tgt = ByteLevelBPETokenizer()\n",
    "tokenize_tgt.train_from_iterator(\n",
    "    [x[TGT] for x in sents['train']['translation']],\n",
    "    vocab_size=10000, min_frequency=2,\n",
    "    special_tokens=SPECIAL_TOKS)\n",
    "assert PAD_IDX == tokenize_tgt.token_to_id(PAD)\n",
    "from torch import Tensor\n",
    "from typing import Optional, Any\n",
    "class CustomDecoderLayer(nn.TransformerDecoderLayer):\n",
    "    def forward(self, tgt: Tensor, memory: Tensor,\n",
    "            tgt_mask: Optional[Tensor] = None,\n",
    "            memory_mask: Optional[Tensor] = None,\n",
    "            tgt_key_padding_mask: Optional[Tensor] = None\n",
    "            ) -> Tensor:\n",
    "        \"\"\"Like decode but returns multi-head attention weights.\"\"\"\n",
    "        tgt2 = self.self_attn(\n",
    "            tgt, tgt, tgt, attn_mask=tgt_mask,\n",
    "            key_padding_mask=tgt_key_padding_mask)[0]\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        tgt = self.norm1(tgt)\n",
    "        tgt2, attention_weights = self.multihead_attn(\n",
    "            tgt, memory, memory,  # <1>\n",
    "            attn_mask=memory_mask,\n",
    "            key_padding_mask=mem_key_padding_mask,\n",
    "            need_weights=True)\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        tgt = self.norm2(tgt)\n",
    "        tgt2 = self.linear2(\n",
    "            self.dropout(self.activation(self.linear1(tgt))))\n",
    "        tgt = tgt + self.dropout3(tgt2)\n",
    "        tgt = self.norm3(tgt)\n",
    "        return tgt, attention_weights  # <2>\n",
    "class CustomDecoder(nn.TransformerDecoder):\n",
    "    def __init__(self, decoder_layer, num_layers, norm=None):\n",
    "        super().__init__(\n",
    "            decoder_layer, num_layers, norm)\n",
    "    def forward(self,\n",
    "            tgt: Tensor, memory: Tensor,\n",
    "            tgt_mask: Optional[Tensor] = None,\n",
    "            memory_mask: Optional[Tensor] = None,\n",
    "            tgt_key_padding_mask: Optional[Tensor] = None\n",
    "            ) -> Tensor:\n",
    "        \"\"\"Like TransformerDecoder but cache multi-head attention\"\"\"\n",
    "        self.attention_weights = []  # <1>\n",
    "        output = tgt\n",
    "        for mod in self.layers:\n",
    "            output, attention = mod(\n",
    "                output, memory, tgt_mask=tgt_mask,\n",
    "                memory_mask=memory_mask,\n",
    "                tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "            self.attention_weights.append(attention) # <2>\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "        return output\n",
    "from einops import rearrange  # <1>\n",
    "class TranslationTransformer(nn.Transformer):  # <2>\n",
    "    def __init__(self,\n",
    "            device=DEVICE,\n",
    "            src_vocab_size: int = VOCAB_SIZE,\n",
    "            src_pad_idx: int = PAD_IDX,\n",
    "            tgt_vocab_size: int = VOCAB_SIZE,\n",
    "            tgt_pad_idx: int = PAD_IDX,\n",
    "            max_sequence_length: int = 100,\n",
    "            d_model: int = 512,\n",
    "            nhead: int = 8,\n",
    "            num_encoder_layers: int = 6,\n",
    "            num_decoder_layers: int = 6,\n",
    "            dim_feedforward: int = 2048,\n",
    "            dropout: float = 0.1,\n",
    "            activation: str = \"relu\"\n",
    "        ):\n",
    "        decoder_layer = CustomDecoderLayer(\n",
    "            d_model, nhead, dim_feedforward,  # <3>\n",
    "            dropout, activation)\n",
    "        decoder_norm = nn.LayerNorm(d_model)\n",
    "        decoder = CustomDecoder(\n",
    "            decoder_layer, num_decoder_layers,\n",
    "            decoder_norm)  # <4>\n",
    "        super().__init__(\n",
    "            d_model=d_model, nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout, custom_decoder=decoder)\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.tgt_pad_idx = tgt_pad_idx\n",
    "        self.device = device\n",
    "        self.src_emb = nn.Embedding(\n",
    "            src_vocab_size, d_model)  # <5>\n",
    "        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.pos_enc = PositionalEncoding(\n",
    "            d_model, dropout, max_sequence_length)  # <6>\n",
    "        self.linear = nn.Linear(\n",
    "            d_model, tgt_vocab_size)  # <7>\n",
    "    def _make_key_padding_mask(self, t, pad_idx):\n",
    "        mask = (t == pad_idx).to(self.device)\n",
    "        return mask\n",
    "    def prepare_src(self, src, src_pad_idx):\n",
    "        src_key_padding_mask = self._make_key_padding_mask(\n",
    "            src, src_pad_idx)\n",
    "        src = rearrange(src, 'N S -> S N')\n",
    "        src = self.pos_enc(self.src_emb(src)\n",
    "            * math.sqrt(self.d_model))\n",
    "        return src, src_key_padding_mask\n",
    "    def prepare_tgt(self, tgt, tgt_pad_idx):\n",
    "        tgt_key_padding_mask = self._make_key_padding_mask(\n",
    "            tgt, tgt_pad_idx)\n",
    "        tgt = rearrange(tgt, 'N T -> T N')\n",
    "        tgt_mask = self.generate_square_subsequent_mask(\n",
    "            tgt.shape[0]).to(self.device)\n",
    "        tgt = self.pos_enc(self.tgt_emb(tgt)\n",
    "            * math.sqrt(self.d_model))\n",
    "        return tgt, tgt_key_padding_mask, tgt_mask\n",
    "    def forward(self, src, tgt):\n",
    "        src, src_key_padding_mask = self.prepare_src(\n",
    "            src, self.src_pad_idx)\n",
    "        tgt, tgt_key_padding_mask, tgt_mask = self.prepare_tgt(\n",
    "            tgt, self.tgt_pad_idx)\n",
    "        memory_key_padding_mask = src_key_padding_mask.clone()\n",
    "        output = super().forward(\n",
    "            src, tgt, tgt_mask=tgt_mask,\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            memory_key_padding_mask=memory_key_padding_mask)\n",
    "        output = rearrange(output, 'T N E -> N T E')\n",
    "        return self.linear(output)\n",
    "    def init_weights(self):\n",
    "        def _init_weights(m):\n",
    "            if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "                nn.init.xavier_uniform_(m.weight.data)\n",
    "        self.apply(_init_weights);  # <1>\n",
    "class TranslationTransformer(nn.Transformer):\n",
    "    def __init__(self,\n",
    "            device=DEVICE,\n",
    "            src_vocab_size: int = 10000,\n",
    "            src_pad_idx: int = PAD_IDX,\n",
    "            tgt_vocab_size: int  = 10000,\n",
    "            tgt_pad_idx: int = PAD_IDX,\n",
    "            max_sequence_length: int = 100,\n",
    "            d_model: int = 512,\n",
    "            nhead: int = 8,\n",
    "            num_encoder_layers: int = 6,\n",
    "            num_decoder_layers: int = 6,\n",
    "            dim_feedforward: int = 2048,\n",
    "            dropout: float = 0.1,\n",
    "            activation: str = \"relu\"\n",
    "            ):\n",
    "        decoder_layer = CustomDecoderLayer(\n",
    "            d_model, nhead, dim_feedforward,\n",
    "            dropout, activation)\n",
    "        decoder_norm = nn.LayerNorm(d_model)\n",
    "        decoder = CustomDecoder(\n",
    "            decoder_layer, num_decoder_layers, decoder_norm)\n",
    "        super().__init__(\n",
    "            d_model=d_model, nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout, custom_decoder=decoder)\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.tgt_pad_idx = tgt_pad_idx\n",
    "        self.device = device\n",
    "        self.src_emb = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.pos_enc = PositionalEncoding(\n",
    "            d_model, dropout, max_sequence_length)\n",
    "        self.linear = nn.Linear(d_model, tgt_vocab_size)\n",
    "    def init_weights(self):\n",
    "        def _init_weights(m):\n",
    "            if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "                nn.init.xavier_uniform_(m.weight.data)\n",
    "        self.apply(_init_weights);\n",
    "    def _make_key_padding_mask(self, t, pad_idx=PAD_IDX):\n",
    "        mask = (t == pad_idx).to(self.device)\n",
    "        return mask\n",
    "    def prepare_src(self, src, src_pad_idx):\n",
    "        src_key_padding_mask = self._make_key_padding_mask(\n",
    "            src, src_pad_idx)\n",
    "        src = rearrange(src, 'N S -> S N')\n",
    "        src = self.pos_enc(self.src_emb(src)\n",
    "            * math.sqrt(self.d_model))\n",
    "        return src, src_key_padding_mask\n",
    "    def prepare_tgt(self, tgt, tgt_pad_idx):\n",
    "        tgt_key_padding_mask = self._make_key_padding_mask(\n",
    "            tgt, tgt_pad_idx)\n",
    "        tgt = rearrange(tgt, 'N T -> T N')\n",
    "        tgt_mask = self.generate_square_subsequent_mask(\n",
    "            tgt.shape[0]).to(self.device)      # <1>\n",
    "        tgt = self.pos_enc(self.tgt_emb(tgt)\n",
    "            * math.sqrt(self.d_model))\n",
    "        return tgt, tgt_key_padding_mask, tgt_mask\n",
    "    def forward(self, src, tgt):\n",
    "        src, src_key_padding_mask = self.prepare_src(\n",
    "            src, self.src_pad_idx)\n",
    "        tgt, tgt_key_padding_mask, tgt_mask = self.prepare_tgt(\n",
    "            tgt, self.tgt_pad_idx)\n",
    "        memory_key_padding_mask = src_key_padding_mask.clone()\n",
    "        output = super().forward(\n",
    "            src, tgt, tgt_mask=tgt_mask,\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            memory_key_padding_mask = memory_key_padding_mask,\n",
    "            )\n",
    "        output = rearrange(output, 'T N E -> N T E')\n",
    "        return self.linear(output)\n",
    "model = TranslationTransformer(\n",
    "    device=DEVICE,\n",
    "    src_vocab_size=tokenize_src.get_vocab_size(),\n",
    "    src_pad_idx=tokenize_src.token_to_id('<pad>'),\n",
    "    tgt_vocab_size=tokenize_tgt.get_vocab_size(),\n",
    "    tgt_pad_idx=tokenize_tgt.token_to_id('<pad>')\n",
    "    ).to(DEVICE)\n",
    "model.init_weights()\n",
    "model  # <1>\n",
    "src = torch.randint(1, 100, (10, 5)).to(DEVICE)  # <1>\n",
    "tgt = torch.randint(1, 100, (10, 7)).to(DEVICE)\n",
    "with torch.no_grad():\n",
    "    output = model(src, tgt)  # <2>\n",
    "print(output.shape)\n",
    "LEARNING_RATE = 0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)  # <1>\n",
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()  # <1>\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(iterator):\n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg[:,:-1])  # <2>\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        trg = trg[:,1:].contiguous().view(-1)\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()  # <1>\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():  # <2>\n",
    "        for i, batch in enumerate(iterator):\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "            output = model(src, trg[:,:-1])\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "N_EPOCHS = 15\n",
    "CLIP = 1\n",
    "BEST_MODEL_FILE = 'best_model.pytorch'\n",
    "best_valid_loss = float('inf')\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    train_loss = train(\n",
    "        model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), BEST_MODEL_FILE)\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    train_ppl = f'{math.exp(train_loss):7.3f}'\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {train_ppl}')\n",
    "    valid_ppl = f'{math.exp(valid_loss):7.3f}'\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {valid_ppl}')\n",
    "model.load_state_dict(torch.load(BEST_MODEL_FILE))\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')\n",
    "def translate_sentence(sentence, src_field, trg_field,\n",
    "        model, device=DEVICE, max_len=50):\n",
    "    model.eval()\n",
    "    if isinstance(sentence, str):\n",
    "        nlp = spacy.load('de')\n",
    "        tokens = [token.text.lower() for token in nlp(sentence)]\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "    tokens = ([src_field.init_token] + tokens\n",
    "        + [src_field.eos_token])  # <1>\n",
    "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
    "    src = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
    "    src, src_key_padding_mask = model.prepare_src(src, SRC_PAD_IDX)\n",
    "    with torch.no_grad():\n",
    "        enc_src = model.encoder(src,\n",
    "            src_key_padding_mask=src_key_padding_mask)\n",
    "    trg_indexes = [\n",
    "        trg_field.vocab.stoi[trg_field.init_token]]  # <2>\n",
    "    for i in range(max_len):\n",
    "        tgt = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
    "        tgt, tgt_key_padding_mask, tgt_mask = model.prepare_tgt(\n",
    "            tgt, TRG_PAD_IDX)\n",
    "        with torch.no_grad():\n",
    "            output = model.decoder(\n",
    "                tgt, enc_src, tgt_mask=tgt_mask,\n",
    "                tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "            output = rearrange(output, 'T N E -> N T E')\n",
    "            output = model.linear(output)\n",
    "        pred_token = output.argmax(2)[:,-1].item()  # <3>\n",
    "        trg_indexes.append(pred_token)\n",
    "        if pred_token == trg_field.vocab.stoi[\n",
    "                trg_field.eos_token]:  # <4>\n",
    "            break\n",
    "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
    "    translation = trg_tokens[1:]\n",
    "    return translation, model.decoder.attention_weights\n",
    "example_idx = 10\n",
    "src = vars(test_data.examples[example_idx])['src']\n",
    "trg = vars(test_data.examples[example_idx])['trg']\n",
    "src\n",
    "trg\n",
    "translation, attention = translate_sentence(src, SRC, TRG, model, device)\n",
    "print(f'translation = {translation}')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "def display_attention(sentence, translation, attention_weights):\n",
    "    n_attention = len(attention_weights)\n",
    "    n_cols = 2\n",
    "    n_rows = n_attention // n_cols + n_attention % n_cols\n",
    "    fig = plt.figure(figsize=(15,25))\n",
    "    for i in range(n_attention):\n",
    "        attention = attention_weights[i].squeeze(0)\n",
    "        attention = attention.cpu().detach().numpy()\n",
    "        cax = ax.matshow(attention, cmap='gist_yarg')\n",
    "        ax = fig.add_subplot(n_rows, n_cols, i+1)\n",
    "        ax.tick_params(labelsize=12)\n",
    "        ax.set_xticklabels([''] + ['<sos>'] +\n",
    "            [t.lower() for t in sentence]+['<eos>'],\n",
    "            rotation=45)\n",
    "        ax.set_yticklabels(['']+translation)\n",
    "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "display_attention(src, translation, attention_weights)\n",
    "example_idx = 25\n",
    "src = vars(valid_data.examples[example_idx])['src']\n",
    "trg = vars(valid_data.examples[example_idx])['trg']\n",
    "print(f'src = {src}')\n",
    "print(f'trg = {trg}')\n",
    "translation, attention = translate_sentence(src, SRC, TRG, model, device)\n",
    "print(f'translation = {translation}')\n",
    "display_attention(src, translation, attention)\n",
    "from torchtext.data.metrics import bleu_score\n",
    "def calculate_bleu(data, src_field, trg_field, model, device, max_len = 50):\n",
    "    trgs = []\n",
    "    pred_trgs = []\n",
    "    for datum in data:\n",
    "        src = vars(datum)['src']\n",
    "        trg = vars(datum)['trg']\n",
    "        pred_trg, _ = translate_sentence(\n",
    "            src, src_field, trg_field, model, device, max_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6c675f",
   "metadata": {},
   "source": [
    "strip <eos> token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecaa4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "        pred_trg = pred_trg[:-1]\n",
    "        pred_trgs.append(pred_trg)\n",
    "        trgs.append([trg])\n",
    "    return bleu_score(pred_trgs, trgs)\n",
    "bleu_score = calculate_bleu(test_data, SRC, TRG, model, device)\n",
    "print(f'BLEU score = {bleu_score*100:.2f}')\n",
    "from transformers import BertModel\n",
    "model = BertModel.from_pre-trained('bert-base-uncased')\n",
    "print(model)\n",
    "import pandas as pd\n",
    "df = pd.read_csv('data/train.csv')  # <1>\n",
    "df.head()\n",
    "df.shape\n",
    "from sklearn.model_selection import train_test_split\n",
    "random_state=42\n",
    "labels = ['toxic', 'severe', 'obscene', 'threat', 'insult', 'hate']\n",
    "X = df[['comment_text']]\n",
    "y = df[labels]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2,\n",
    "    random_state=random_state)  # <1>\n",
    "def get_dataset(X, y):\n",
    "    data = [[X.iloc[i][0], y.iloc[i].values.tolist()] for i in range(X.shape[0])]\n",
    "    return pd.DataFrame(data, columns=['text', 'labels'])\n",
    "train_df = get_dataset(X_train, y_train)\n",
    "eval_df = get_dataset(X_test, y_test)\n",
    "train_df.shape, eval_df.shape\n",
    "train_df.head()  # <1>\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)  # <1>\n",
    "model_type = 'bert'  # <2>\n",
    "model_name = 'bert-base-cased'\n",
    "output_dir = f'{model_type}-example1-outputs'\n",
    "model_args = {\n",
    "    'output_dir': output_dir, # where to save results\n",
    "    'overwrite_output_dir': True, # allow re-run without having to manually clear output_dir\n",
    "    'manual_seed': random_state, # <3>\n",
    "    'no_cache': True,\n",
    "}\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from simpletransformers.classification import MultiLabelClassificationModel\n",
    "model = MultiLabelClassificationModel(\n",
    "    model_type, model_name, num_labels=len(labels),\n",
    "    args=model_args)\n",
    "model.train_model(train_df=train_df)  # <1>\n",
    "result, model_outputs, wrong_predictions = model.eval_model(eval_df,\n",
    "    acc=roc_auc_score)  # <1>\n",
    "result\n",
    "from preprocessing.preprocessing import TextPreprocessor\n",
    "tp = TextPreprocessor()\n",
    "df = df.rename(columns={'comment_text':'original_text'})\n",
    "df['comment_text'] = df['original_text'].apply(\n",
    "    lambda x: tp.preprocess(x))  # <1>\n",
    "pd.set_option('display.max_colwidth', 45)\n",
    "df[['original_text', 'comment_text']].head()\n",
    "model_type = 'bert'\n",
    "model_name = 'bert-base-cased'\n",
    "output_dir = f'{model_type}-example2-outputs'  # <1>\n",
    "best_model_dir = f'{output_dir}/best_model'\n",
    "model_args = {\n",
    "    'output_dir': output_dir,\n",
    "    'overwrite_output_dir': True,\n",
    "    'manual_seed': random_state,\n",
    "    'no_cache': True,\n",
    "    'best_model_dir': best_model_dir,\n",
    "    'max_seq_length': 300,\n",
    "    'train_batch_size': 24,\n",
    "    'eval_batch_size': 24,\n",
    "    'gradient_accumulation_steps': 1,\n",
    "    'learning_rate': 5e-5,\n",
    "    'evaluate_during_training': True,\n",
    "    'evaluate_during_training_steps': 1000,\n",
    "    'save_eval_checkpoints': False,\n",
    "    \"save_model_every_epoch\": False,\n",
    "    'save_steps': -1,  # saving model unnecessarily takes time during training\n",
    "    'reprocess_input_data': True,\n",
    "    'num_train_epochs': 5,  # <2>\n",
    "    'use_early_stopping': True,\n",
    "    'early_stopping_patience': 4,  # <3>\n",
    "    'early_stopping_delta': 0,\n",
    "}\n",
    "model = MultiLabelClassificationModel(\n",
    "    model_type, model_name, num_labels=len(labels),\n",
    "    args=model_args)\n",
    "model.train_model(\n",
    "    train_df=train_df, eval_df=eval_df, acc=roc_auc_score,\n",
    "    show_running_loss=False, verbose=False)\n",
    "best_model = MultiLabelClassificationModel(\n",
    "    model_type, best_model_dir,\n",
    "    num_labels=len(labels), args=model_args)\n",
    "result, model_outputs, wrong_predictions = best_model.eval_model(\n",
    "    eval_df, acc=roc_auc_score)\n",
    "result\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
