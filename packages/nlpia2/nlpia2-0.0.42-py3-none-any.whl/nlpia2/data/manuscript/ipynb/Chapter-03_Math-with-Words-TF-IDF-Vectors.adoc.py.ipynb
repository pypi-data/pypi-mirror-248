{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a59c140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "sentence = ('It has also arisen in criminal justice, healthcare, and '\n",
    "    'hiring, compounding existing racial, economic, and gender biases.')\n",
    "doc = nlp(sentence)\n",
    "tokens = [token.text for token in doc]\n",
    "tokens\n",
    "from collections import Counter\n",
    "bag_of_words = Counter(tokens)\n",
    "bag_of_words\n",
    "bag_of_words.most_common(3)  # <2>\n",
    "counts = pd.Series(dict(bag_of_words.most_common()))  # <1>\n",
    "counts\n",
    "len(tokens)\n",
    "counts.sum()\n",
    "counts / counts.sum()  # <3>\n",
    "counts['justice']\n",
    "counts['justice'] / counts.sum()\n",
    "sentence = \"Algorithmic bias has been cited in cases ranging from \" \\\n",
    "    \"election outcomes to the spread of online hate speech.\"\n",
    "tokens = [tok.text for tok in nlp(sentence)]\n",
    "counts = Counter(tokens)\n",
    "counts\n",
    "import requests\n",
    "url = ('https://gitlab.com/tangibleai/nlpia2/'\n",
    "       '-/raw/main/src/nlpia2/ch03/bias_intro.txt')\n",
    "response = requests.get(url)\n",
    "response\n",
    "bias_intro_bytes = response.content  # <1>\n",
    "bias_intro = response.text  # <2>\n",
    "assert bias_intro_bytes.decode() == bias_intro    # <3>\n",
    "bias_intro[:60]\n",
    "tokens = [tok.text for tok in nlp(bias_intro)]\n",
    "counts = Counter(tokens)\n",
    "counts\n",
    "counts.most_common(5)\n",
    "counts.most_common()[-4:]\n",
    "counts.most_common()[-4:]\n",
    "docs = [nlp(s) for s in bias_intro.split('\\n') if s.strip()]  # <1>\n",
    "counts = []\n",
    "for doc in docs:\n",
    "    counts.append(Counter([t.text.lower() for t in doc]))  # <2>\n",
    "df = pd.DataFrame(counts)\n",
    "df = df.fillna(0).astype(int)  # <3>\n",
    "df.head()\n",
    "df.loc[10]  # <1>\n",
    "docs = list(nlp(bias_intro).sents)\n",
    "counts = []\n",
    "for doc in docs:\n",
    "    counts.append(Counter([t.text.lower() for t in doc]))\n",
    "df = pd.DataFrame(counts)\n",
    "df = df.fillna(0).astype(int)  # <1>\n",
    "df\n",
    "docs_tokens = []\n",
    "for doc in docs:\n",
    "    doc_text = doc.text.lower()  # <1>\n",
    "    docs_tokens.append([tok.text for tok in nlp(doc_text)])\n",
    "len(docs_tokens[0])\n",
    "all_doc_tokens = []\n",
    "for doc_tokens in docs_tokens:\n",
    "    all_doc_tokens.extend(doc_tokens)\n",
    "len(all_doc_tokens)\n",
    "vocab = sorted(set(all_doc_tokens))\n",
    "len(vocab)\n",
    "vocab\n",
    "from collections import OrderedDict\n",
    "zero_vector = OrderedDict((token, 0) for token in lexicon)\n",
    "list(zero_vector.items())[:10]  # <1>\n",
    "import copy\n",
    "doc_vectors = []\n",
    "for doc in docs:\n",
    "    vec = copy.copy(zero_vector)  # <1>\n",
    "    tokens = [token.text for token in nlp(doc.lower())]\n",
    "    token_counts = Counter(tokens)\n",
    "    for key, value in token_counts.items():\n",
    "        vec[key] = value / len(lexicon)\n",
    "    doc_vectors.append(vec)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [doc.text for doc in docs]\n",
    "vectorizer = CountVectorizer()\n",
    "count_vectors = vectorizer.fit_transform(corpus)  # <1>\n",
    "print(count_vectors.toarray()) # <2>\n",
    "v1 = np.array(list(range(5)))\n",
    "v2 = pd.Series(reversed(range(5)))\n",
    "slow_answer = sum([4.2 * (x1 * x2) for x1, x2 in zip(v1, v2)])\n",
    "slow_answer\n",
    "faster_answer = sum(4.2 * v1 * v2)  # <1>\n",
    "faster_answer\n",
    "fastest_answer = 4.2 * v1.dot(v2)  # <2>\n",
    "fastest_answer\n",
    "import math\n",
    "def cosine_sim(vec1, vec2):\n",
    "    vec1 = [val for val in vec1.values()] # <1>\n",
    "    vec2 = [val for val in vec2.values()]\n",
    "    dot_prod = 0\n",
    "    for i, v in enumerate(vec1):\n",
    "        dot_prod += v * vec2[i]\n",
    "    mag_1 = math.sqrt(sum([x**2 for x in vec1]))\n",
    "    mag_2 = math.sqrt(sum([x**2 for x in vec2]))\n",
    "    return dot_prod / (mag_1 * mag_2)\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "vec1 = count_vectors[1,:]\n",
    "vec2 = count_vectors[2,:]\n",
    "cosine_similarity(vec1, vec2)\n",
    "new_sentence = \"What is algorithmic bias?\"\n",
    "ngram_docs = copy.copy(docs)\n",
    "ngram_docs.append(new_sentence)\n",
    "new_sentence_vector = vectorizer.transform([new_sentence])\n",
    "print(new_sentence_vector.toarray())\n",
    "cosine_similarity(count_vectors[1,:], new_sentence)\n",
    "ngram_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "ngram_vectors = ngram_vectorizer.fit_transform(corpus)\n",
    "print(ngram_vectors.toarray())\n",
    "cosine_similarity(ngram_vectors[1,:], ngram_vectors[2,:])\n",
    "from this import s\n",
    "print (s)\n",
    "char_vectorizer = CountVectorizer(\n",
    "    ngram_range=(1,1), analyzer='char')  # <1>\n",
    "s_char_frequencies = char_vectorizer.fit_transform(s)\n",
    "generate_histogram(\n",
    "    s_char_frequencies, s_char_vectorizer)  # <2>\n",
    "DATA_DIR = ('https://gitlab.com/tangibleai/nlpia/'\n",
    "            '-/raw/master/src/nlpia/data')\n",
    "url = DATA_DIR + '/machine_learning_full_article.txt'\n",
    "ml_text = requests.get(url).content.decode()\n",
    "ml_char_frequencies = char_vectorizer.fit_transform(ml_text)\n",
    "generate_histogram(s_char_frequencies, s_char_vectorizer)\n",
    "peak_distance = ord('R') - ord('E')\n",
    "peak_distance\n",
    "chr(ord('v') - peak_distance)  # <1>\n",
    "chr(ord('n') - peak_distance)  # <2>\n",
    "chr(ord('W') - peak_distance)\n",
    "import codecs\n",
    "print(codecs.decode(s, 'rot-13'))\n",
    "nltk.download('brown')  # <1>\n",
    "from nltk.corpus import brown\n",
    "brown.words()[:10]  # <2>\n",
    "brown.tagged_words()[:5]  # <3>\n",
    "len(brown.words())\n",
    "from collections import Counter\n",
    "puncs = set((',', '.', '--', '-', '!', '?',\n",
    "    ':', ';', '``', \"''\", '(', ')', '[', ']'))\n",
    "word_list = (x.lower() for x in brown.words() if x not in puncs)\n",
    "token_counts = Counter(word_list)\n",
    "token_counts.most_common(10)\n",
    "DATA_DIR = ('https://gitlab.com/tangibleai/nlpia/'\n",
    "            '-/raw/master/src/nlpia/data')\n",
    "url = DATA_DIR + '/bias_discrimination.txt'\n",
    "bias_discrimination = requests.get(url).content.decode()\n",
    "intro_tokens = [token.text for token in nlp(bias_intro.lower())]\n",
    "disc_tokens = [token.text for token in nlp(bias_discrimination.lower())]\n",
    "intro_total = len(intro_tokens)\n",
    "intro_total\n",
    "disc_total = len (disc_tokens)\n",
    "disc_total\n",
    "intro_tf = {}\n",
    "disc_tf = {}\n",
    "intro_counts = Counter(intro_tokens)\n",
    "intro_tf['bias'] = intro_counts['bias'] / intro_total\n",
    "disc_counts = Counter(disc_tokens)\n",
    "disc_tf['bias'] = disc_counts['bias'] / disc_total\n",
    "'Term Frequency of \"bias\" in intro is:{:.4f}'.format(intro_tf['bias'])\n",
    "'Term Frequency of \"bias\" in discrimination chapter is: {:.4f}'\\\n",
    "    .format(disc_tf['bias'])\n",
    "intro_tf['and'] = intro_counts['and'] / intro_total\n",
    "disc_tf['and'] = disc_counts['and'] / disc_total\n",
    "print('Term Frequency of \"and\" in intro is: {:.4f}'\\\n",
    "    .format(intro_tf['and']))\n",
    "print('Term Frequency of \"and\" in discrimination chapter is: {:.4f}'\\\n",
    "    .format(disc_tf['and']))\n",
    "num_docs_containing_and = 0\n",
    "for doc in [intro_tokens, disc_tokens]:\n",
    "    if 'and' in doc:\n",
    "        num_docs_containing_and += 1  # <1>\n",
    "intro_tf['black'] = intro_counts['black'] / intro_total\n",
    "disc_tf['black'] = disc_counts['black'] / disc_total\n",
    "num_docs = 2\n",
    "intro_idf = {}\n",
    "disc_idf = {}\n",
    "intro_idf['and'] = num_docs / num_docs_containing_and\n",
    "disc_idf['and'] = num_docs / num_docs_containing_and\n",
    "intro_idf['bias'] = num_docs / num_docs_containing_bias\n",
    "disc_idf['bias'] = num_docs / num_docs_containing_bias\n",
    "intro_idf['black'] = num_docs / num_docs_containing_black\n",
    "disc_idf['black'] = num_docs / num_docs_containing_black\n",
    "intro_tfidf = {}\n",
    "intro_tfidf['and'] = intro_tf['and'] * intro_idf['and']\n",
    "intro_tfidf['bias'] = intro_tf['bias'] * intro_idf['bias']\n",
    "intro_tfidf['black'] = intro_tf['black'] * intro_idf['black']\n",
    "disc_tfidf = {}\n",
    "disc_tfidf['and'] = disc_tf['and'] * disc_idf['and']\n",
    "disc_tfidf['bias'] = disc_tf['bias'] * disc_idf['bias']\n",
    "disc_tfidf['black'] = disc_tf['black'] * disc_idf['black']\n",
    "log_tf = log(term_occurences_in_doc) -\\\n",
    "    log(num_terms_in_doc)  # <1>\n",
    "log_log_idf = log(log(total_num_docs) -\\\n",
    "    log(num_docs_containing_term))  # <2>\n",
    "log_tf_idf = log_tf + log_log_idf  # <3>\n",
    "doc_tfidf_vectors = []\n",
    "for doc in docs:  # <1>\n",
    "    vec = copy.copy(zero_vector)  # <2>\n",
    "    tokens = [token.text for token in nlp(doc.lower())]\n",
    "    token_counts = Counter(tokens)\n",
    "    for token, count in token_counts.items():\n",
    "        docs_containing_key = 0\n",
    "        for d in docs:\n",
    "            if token in d:\n",
    "                docs_containing_key += 1\n",
    "        tf = value / len(lexicon)\n",
    "        if docs_containing_key:\n",
    "            idf = len(docs) / docs_containing_key\n",
    "        else:\n",
    "            idf = 0\n",
    "        vec[key] = tf * idf\n",
    "    doc_tfidf_vectors.append(vec)\n",
    "query = \"How long does it take to get to the store?\"\n",
    "query_vec = copy.copy(zero_vector)  # <1>\n",
    "tokens = [token.text for token in nlp(query.lower())]\n",
    "token_counts = Counter(tokens)\n",
    "for key, value in token_counts.items():\n",
    "    docs_containing_key = 0\n",
    "    for _doc in docs:\n",
    "      if key in _doc.lower():\n",
    "        docs_containing_key += 1\n",
    "    if docs_containing_key == 0:  # <1>\n",
    "        continue\n",
    "    tf = value / len(tokens)\n",
    "    idf = len(docs) / docs_containing_key\n",
    "    query_vec[key] = tf * idf\n",
    "cosine_sim(query_vec, doc_tfidf_vectors[0])\n",
    "cosine_sim(query_vec, doc_tfidf_vectors[1])\n",
    "cosine_sim(query_vec, doc_tfidf_vectors[2])\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = docs\n",
    "vectorizer = TfidfVectorizer(min_df=1) # <1>\n",
    "vectorizer = vectorizer.fit(corpus)  # <2>\n",
    "vectors = vectorizer.transform(corpus)  # <3>\n",
    "print(vectors.todense().round(2))  # <4>\n",
    "DS_FAQ_URL = ('https://gitlab.com/tangibleai/qary/-/raw/main/'\n",
    "qa_dataset = pd.read_csv(DS_FAQ_URL)\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(df['question'])\n",
    "tfidfvectors_sparse = vectorizer.transform(df['question']) #<1>\n",
    "tfidfvectors = tfidfvectors_sparse.todense() #<2>\n",
    "def bot_reply(question):\n",
    "   question_vector = vectorizer.transform([question]).todense()\n",
    "   idx = question_vector.dot(tfidfvectors.T).argmax() # <1>\n",
    "   print(\n",
    "       f\"Your question:\\n  {question}\\n\\n\"\n",
    "       f\"Most similar FAQ question:\\n  {df['question'][idx]}\\n\\n\"\n",
    "       f\"Answer to that FAQ question:\\n  {df['answer'][idx]}\\n\\n\"\n",
    "   )\n",
    "bot_reply(\"What's overfitting a model?\")\n",
    "bot_reply('How do I decrease overfitting for Logistic Regression?')\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
